---
title: Bayesian errors-in-variables regression with Gibbs sampling in R
author: ''
date: '2023-04-02'
slug: bayesian-errors-in-variables-regression-with-gibbs-sampling-in-r
categories: [R, Bayesian, JAGS, Gibbs sampling]
tags: []
subtitle: ''
authors: []
#featured: false
draft: true
image:
  caption: ''
  focal_point: ''
  preview_only: true
projects: []
header-includes: \usepackage{graphics}
summary: Errors-in-variables regression is preferable to simple linear regression when the observations come with uncertainty. Here, I first show how errors-in-variables regression can be done in R, using JAGS. I then construct a Gibbs sampler that runs purely in R, without any external software like JAGS or Stan. 

---




<style>
.math {
  font-size: small;
}
</style>
<style>
<p>.column-left{
float: left;
width: 52%;
text-align: left;</p>
<p>}</p>
<style>
.column-center{
  float: center;
  width: 100%;
  text-align: left;

}

.column-right{
  float: right;
  width: 48%;
  text-align: left;
  margin-top: 6px;
  line-height: 1.83;
  font-size: 12px;

}
</style>
<style type="text/css">
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 13.2px
}
</style>
<p>Assume we want to conduct a linear regression of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>. Both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> come with normally distributed uncertainties or measurement errors, which we need to take into consideration, or we risk obtaining biased regression coefficients. Let’s generate <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, with uncertainties denoted as standard deviations <span class="math inline">\(sigma_x\)</span> and <span class="math inline">\(sigma_y\)</span>. Each point may have a different uncertainty. Below, we generate some sample data.</p>
<pre class="r"><code>set.seed(1) # change the seed for a different sequence of random numbers
n &lt;- 30 # number of total data points
x &lt;- runif(n,-2,2) # generate the true x
y &lt;- 2 + 0.75 * x # generate the true y
# define the standard deviations of the normal uncertainties with which x and y where observed
sigma_x &lt;- runif(n,0.2,0.5)
sigma_y &lt;- runif(n,0.2,0.8)
# generate observations from x and y, given these uncertainties
x_obs &lt;- rnorm(n,x,sigma_x)
y_obs &lt;- rnorm(n,y,sigma_y)</code></pre>
<p>Now, we show the observations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, along with the true underlying relationship. We also add a simple linear regression, using the R functions <code>lm()</code> and `predict.lm(), to compare it to the true relationship. The shaded area denotes the 95% confidence interval of the regression:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="384" /></p>
<p>Next, we build a Bayesian, hierarchical model that accounts for the uncertainties around our observations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. It starts like a simple linear regression model, with intercept <span class="math inline">\(\alpha\)</span>, slope <span class="math inline">\(\beta\)</span>, and residual standard deviation <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[y_{est} \sim N(\mu, \sigma)\]</span>
<span class="math display">\[\mu = \alpha + \beta x_{est}\]</span>
However, instead of using <span class="math inline">\(x_{obs}\)</span> and <span class="math inline">\(y_{obs}\)</span> directly, we let the model estimate the true values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, i.e. <span class="math inline">\(x_{est}\)</span> and <span class="math inline">\(y_{est}\)</span>:</p>
<p><span class="math display">\[x_{obs} \sim N(x_{est}, \sigma_x)\]</span>
<span class="math display">\[y_{obs} \sim N(y_{est}, \sigma_y)\]</span></p>
<p>To get the posterior of this model, we use Markov Chain Monte Carlo methods, implemented in the JAGS software. We can almost directly pass the model formulas to JAGS using the <code>R2jags</code> package, so this is quite straightforward.
Here is the JAGS model, with vague priors on <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma\)</span>:</p>
<pre class="r"><code>errors_in_variables_regression_jags &lt;- function() {
  ## Likelihood
  for (i in 1:n){  
    y_est[i] ~ dnorm(mu[i], tau) # JAGS uses precision `tau` instead of sigma
    mu[i] &lt;- alpha + beta * x_est[i]
    x_est[i] ~ dnorm(x_obs[i], 1/(sigma_x[i]*sigma_x[i])) # precision = 1/sigma^2
    y_obs[i] ~ dnorm(y_est[i], 1/(sigma_y[i]*sigma_y[i])) # precision = 1/sigma^2
  }
  ## Priors
  tau ~ dgamma(1, 1)  # gamma prior for precision
  sigma &lt;- 1/sqrt(tau) # calculate residual standard deviation
  alpha ~ dnorm(0, 1/(10^2)) # normal prior with standard deviation = 10
  beta ~ dnorm(0, 1/(10^2)) # normal prior with standard deviation = 10
}</code></pre>
<p>We load the <code>R2jags</code> package, specify the data used in the model, and send everything to JAGS:</p>
<pre class="r"><code>library(R2jags)

regression_data &lt;- list(&quot;x_obs&quot;, &quot;y_obs&quot;, &quot;sigma_x&quot;, &quot;sigma_y&quot;,&quot;n&quot;)

lm_jags  &lt;- jags(data = regression_data,
                parameters.to.save = c(&quot;alpha&quot;,
                                       &quot;beta&quot;,
                                       &quot;sigma&quot;,
                                       &quot;y_est&quot;,
                                       &quot;x_est&quot;
                ),
                n.iter = 3000,
                n.thin = 1,
                n.chains =  3, # Other values set at default (for simplicity)
                model.file = errors_in_variables_regression_jags)</code></pre>
<p>Let’s visualise this regression and compare it to the simple linear regression from above:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="384" /></p>
<pre><code>## [[1]]
## NULL
## 
## [[2]]
## NULL
## 
## [[3]]
## NULL
## 
## [[4]]
## NULL
## 
## [[5]]
## NULL
## 
## [[6]]
## NULL
## 
## [[7]]
## NULL
## 
## [[8]]
## NULL
## 
## [[9]]
## NULL
## 
## [[10]]
## NULL
## 
## [[11]]
## NULL
## 
## [[12]]
## NULL
## 
## [[13]]
## NULL
## 
## [[14]]
## NULL
## 
## [[15]]
## NULL
## 
## [[16]]
## NULL
## 
## [[17]]
## NULL
## 
## [[18]]
## NULL
## 
## [[19]]
## NULL
## 
## [[20]]
## NULL
## 
## [[21]]
## NULL
## 
## [[22]]
## NULL
## 
## [[23]]
## NULL
## 
## [[24]]
## NULL
## 
## [[25]]
## NULL
## 
## [[26]]
## NULL
## 
## [[27]]
## NULL
## 
## [[28]]
## NULL
## 
## [[29]]
## NULL
## 
## [[30]]
## NULL</code></pre>
<p>The mean regression line is very similar to the simple linear regression, but the 95 % credible interval of the hierarchical model (orange) is wider. This is because it correctly reflects the uncertainty associated with the observations, which the simple linear model ignored. Also, notice how the estimates of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (orange triangles) have been shifted towards the mean regression line - this effect is called shrinkage, and is characteristic of hierarchical models. The estimates <span class="math inline">\(x_{est}\)</span> and <span class="math inline">\(y_{est}\)</span> are essentially a compromise between the inferred linear relationship, and the observed values <span class="math inline">\(x_{est}\)</span> and <span class="math inline">\(y_{est}\)</span>.</p>
<p>So far, so good. Now, we will build a Gibbs sampler that can give us draws from the posterior of this model without relying on JAGS to do the sampling for us. Gibbs Sampling takes advantage of the fact that whilst it may be difficult to sample from the full posterior directly, we can iteratively sample from the conditional posterior of individual parameters given the current values of all other parameters. Deriving the conditional posteriors is easiest when the prior and likelihood functions are conjugate, that is when they belong to the same family of probability distributions.</p>
<p>The posterior of our model looks like this:</p>
<p><span class="math display">\[P(\alpha, \beta, \sigma, x_{est}, y_{est} | x_{obs}, \sigma_x, y_{obs}, \sigma_y) \propto \]</span></p>
