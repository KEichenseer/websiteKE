<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Kilian Eichenseer</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 26 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>A Metropolis algorithm in R - Part 2: Adaptive proposals</title>
      <link>/post/a-metropolis-algorithm-in-r-part-2-adaptive-proposals/</link>
      <pubDate>Sat, 26 Mar 2022 00:00:00 +0000</pubDate>
      <guid>/post/a-metropolis-algorithm-in-r-part-2-adaptive-proposals/</guid>
      <description>



&lt;style&gt;
.math {
  font-size: small;
}
&lt;/style&gt;
&lt;style type=&#34;text/css&#34;&gt;
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 13.2px
}
&lt;/style&gt;
&lt;p&gt;In the &lt;a href=&#34;/post/a-metropolis-algorithm-in-r-part-1-implementation&#34;&gt;previous post&lt;/a&gt;, we built a Metropolis algorithm to estimate latitudinal temperature gradients, approximated by a generalised logistic function. Recall that the Metropolis algorithm works by proposing new parameter values and evaluating the joint posterior probability of the model with these values, against the posterior with the current values.&lt;/p&gt;
&lt;p&gt;How do we chose a new value for a parameter? A common approach is to sample a normal distribution, centred at the current value (i.e. the mean of the distribution is the current value). Choosing the standard deviation of the proposal distribution (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt;) is more tricky. If &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; is too high, we end up proposing a lot of values at the far tail ends of the target posterior distribution, which will usually be rejected (see below, green proposals). This leads to inefficient sampling and patchy coverage of the posterior distribution. Conversely, a very small &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; leads to most new values being accepted, but the resulting Markov chain will move very slowly through the parameter space, leading to a low effective sample size (red proposals below). Instead, some intermediate &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; is desirable, at which the Markov chain moves quickly through the parameter space, without too many rejections (e.g., yellow proposals below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-metropolis-algorithm-in-r-part-2-adaptive-proposals/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It turns out that the Metropolis algorithm is usually most efficient when the acceptance rate of proposals is between $ 0.2$ and &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt; (see &lt;a href=&#34;https://www.jstor.org/stable/pdf/3182776.pdf?casa_token=L8a1gJYi1SgAAAAA:ZVa-bCWzwBW3vAat13KRVDkDRu63BWmdxddvp2xLGAjV0bt1j72SP_tEXsxJrU1GqRDyu_23QMDMnCrMJM9Ydrc3bUAylT9eeJeqs5cmPrk9EFIiq9i-&#34;&gt;Roberts and Rosenthal 2001&lt;/a&gt;). In practice, this could be achieved e.g. by monitoring the acceptance rate or the standard deviation of the target distribution (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{target}\)&lt;/span&gt;), and adjusting &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; accordingly. Here, we will take the latter approach. For a four-dimensional Gaussian target distribution, the optimal &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; should be around &lt;span class=&#34;math inline&#34;&gt;\(~2.4/sqrt(4) \times \sigma_{target}\)&lt;/span&gt; (&lt;a href=&#34;http://people.ee.duke.edu/~lcarin/baystat5.pdf&#34;&gt;Gelman et al. 1996&lt;/a&gt;), so we will adapt &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; towards this target.&lt;/p&gt;
&lt;p&gt;In order to allow for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; to quickly converge on the optimum, the weighted variance of the samples from the Markov chains from previous iterations is used as the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; for the next iteration. The weights decrease backwards in time, so that the recent values have more influence on the new value for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt;. The weighted variance is calculated with the following function, where &lt;code&gt;x&lt;/code&gt; denotes a vector of samples from the Markov chain, weights a vector of &lt;code&gt;weights&lt;/code&gt; (see below), and &lt;code&gt;sum_weights&lt;/code&gt; records the sum of the weights vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weighted_var &amp;lt;- function(x, weights, sum_weights) {
  sum(weights*((x-sum(weights*x)/sum_weights)^2))/(sum_weights)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can re-use most of the auxiliary functions of the &lt;a href=&#34;/post/a-metropolis-algorithm-in-r-part-1-implementation&#34;&gt;standard Metropolis algorithm&lt;/a&gt;. Notably, the value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; (&lt;code&gt;propose_sd&lt;/code&gt;) will change during the adaptation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MH_propose &amp;lt;- function(coeff, proposal_sd){
  rnorm(4,mean = coeff, sd= proposal_sd)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The specification of the weights and the adaption of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; is implemented in the updated MCMC loop. &lt;code&gt;nAdapt&lt;/code&gt; specifies the number of iterations in which adaptations takes place. These iterations need to be discarded as burn-in to not bias the estimate of the posterior. &lt;code&gt;adaptation_decay&lt;/code&gt; is a constant that influences the exponential decay of the weights for the weighted variance function, with larger values leading to slower decay.&lt;/p&gt;
&lt;p&gt;Below is the full MCMC loop:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Main MCMCM function
run_MCMC &amp;lt;- function(x, y, coeff_inits, sdy_init, nIter, proposal_sd_init = rep(5,4), 
                     nAdapt = 5000, adaptation_decay = 500){
  ### Initialisation
  coefficients = array(dim = c(nIter,4)) # set up array to store coefficients
  coefficients[1,] = coeff_inits # initialise coefficients
  sdy = rep(NA_real_,nIter) # set up vector to store sdy
  sdy[1] = sdy_init # intialise sdy
  A_sdy = 3 # parameter for the prior on the inverse gamma distribution of sdy
  B_sdy = 0.1 # parameter for the prior on the inverse gamma distribution of sdy
  n &amp;lt;- length(y)
  shape_sdy &amp;lt;- A_sdy+n/2 # shape parameter for the inverse gamma
  sd_it &amp;lt;- 1 # iteration index for the proposal standard deviation
  proposal_sd &amp;lt;- array(NA_real_,dim = c(nAdapt,4)) # array to store proposal SDs
  proposal_sd[1:3,] &amp;lt;- proposal_sd_init # proposal SDs before adaptation
  # pre-define exp. decaying weights for weighted variance
  allWeights &amp;lt;- exp((-(nAdapt-2)):0/adaptation_decay) 
  accept &amp;lt;- rep(NA,nIter) # vector to store the acceptance or rejection of proposals
  ### The MCMC loop
  for (i in 2:nIter){

   ## 1. Gibbs step to estimate sdy
    sdy[i] = sqrt(1/rgamma(
      1,shape_sdy,B_sdy+0.5*sum((y-gradient(x,coefficients[i-1,],0))^2)))

   ## 2. Metropolis-Hastings step to estimate the regression coefficients
    proposal = MH_propose(coefficients[i-1,],proposal_sd[sd_it,]) # new proposed values
    if(any(proposal[4] &amp;lt;= 0)) HR = 0 else {# Q and nu need to be &amp;gt;0
      # Hasting&amp;#39;s ratio of the proposal
      HR = exp(logposterior(x = x, y = y, coeff = proposal, sdy = sdy[i]) -
                 logposterior(x = x, y = y, coeff = coefficients[i-1,], sdy = sdy[i]))}

    #if(gradient(65, proposal,0) &amp;gt;10) HR = 0
    # accept proposal with probability = min(HR,1)
    if (runif(1) &amp;lt; HR){
      accept[i] &amp;lt;- 1
      coefficients[i,] = proposal
      # if proposal is rejected, keep the values from the previous iteration
    }else{
      accept[i] &amp;lt;- 0
      coefficients[i,] = coefficients[i-1,]
    }
    # Adaptation of proposal SD
    if(i &amp;lt; nAdapt){ # stop adaptation after nAdapt iterations
    if(i&amp;gt;=3) {
    weights = allWeights[(nAdapt-i+2):nAdapt-1] # select weights for current iteration
    sum_weights = sum(weights) 
    weighted_var_coeff &amp;lt;- apply(coefficients[2:i,], 2, # calculate weighted variance
          function(f) weighted_var(
          f, weights = weights, sum_weights = sum_weights))

    for(v in 1:4) {if(weighted_var_coeff[v]==0)   { # 
              proposal_sd[i+1,v] &amp;lt;- sqrt(proposal_sd[i,v]^2/5)
      } else  proposal_sd[i+1,v] &amp;lt;- 2.4/sqrt(4) * sqrt(weighted_var_coeff[v]) + 0.0001
    }                   
                           
    }
    sd_it &amp;lt;- i+1
    }
  } # end of the MCMC loop

  ###  Function output
  output = list(coeff = data.frame(A = coefficients[,1],
                           K = coefficients[,2],
                           M = coefficients[,3],
                           Q = coefficients[,4],
                           sdy = sdy),
                proposal_sd = data.frame(proposal_sd),
                accept = accept)
  return(output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We set the initial standard deviation of the proposals to &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### Taking samples
set.seed(10)
sample_lat &amp;lt;- runif(10,0,90)
sample_data &amp;lt;- data.frame(
  x = sample_lat, 
  y = gradient(x = sample_lat, coeff = c(-2.0, 28, 41, 0.1), sd = 2))

### Analysis
nIter &amp;lt;- 100000
print(system.time({m &amp;lt;- run_MCMC(x = sample_data$x, y = sample_data$y,
                                 coeff_inits = c(0,30,45,0.2), sdy_init = 4, 
                                 nIter = nIter, nAdapt = 5000, adaptation_decay =500,
                                 proposal_sd_init = c(5,5,5,5))}))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##    4.26    0.67    4.94&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the traceplots, the MCMC runs well after an initial burn-in period, in which &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; was adjusted.
&lt;img src=&#34;/post/a-metropolis-algorithm-in-r-part-2-adaptive-proposals/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;691.2&#34; /&gt;
The traceplot below shows how the standard deviations of the proposals were adapted during the first &lt;span class=&#34;math inline&#34;&gt;\(5000\)&lt;/span&gt; iterations. Despite a very bad initial guess for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;, the adaptive algorithm managed to quickly find smaller, better values, resulting in reasonably good behaviour of the Markov chains of the parameters shown above. The effective sample size after adaptation is &lt;span class=&#34;math inline&#34;&gt;\(3720\)&lt;/span&gt;, which is &lt;span class=&#34;math inline&#34;&gt;\(52\%\)&lt;/span&gt; higher than the effective sample size obtained with the same number of iterations in the &lt;a href=&#34;/post/a-metropolis-algorithm-in-r-part-1-implementation&#34;&gt;previous version&lt;/a&gt; of this model, where we tried to guess good values for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt;.
&lt;img src=&#34;/post/a-metropolis-algorithm-in-r-part-2-adaptive-proposals/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;691.2&#34; /&gt;
Checking the acceptance rate shows that it is somewhat low even after the adaptation, at &lt;strong&gt;0.16&lt;/strong&gt;. In multi-dimensional models, a good acceptance rate would be around &lt;span class=&#34;math inline&#34;&gt;\(0.25\)&lt;/span&gt; &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-applied-probability/volume-7/issue-1/Weak-convergence-and-optimal-scaling-of-random-walk-Metropolis-algorithms/10.1214/aoap/1034625254.full&#34;&gt;(Gelman et al. 1997)&lt;/a&gt;. To further improve this algorithm and achieve more efficient sampling at a higher acceptance rate, we could use multivariate proposals that account for the correlation of parameters (see e.g. &lt;a href=&#34;https://www.jstor.org/stable/3318737?seq=1#metadata_info_tab_contents&#34;&gt;Haario et al. 2001&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;You can find the full R code to reproduce all analyses and figures on &lt;a href=&#34;https://github.com/KEichenseer/Methods/blob/main/Adaptive_Metropolis-Hastings.R&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Metropolis algorithm in R - Part 1: Implementation</title>
      <link>/post/a-metropolis-algorithm-in-r-part-1-implementation/</link>
      <pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate>
      <guid>/post/a-metropolis-algorithm-in-r-part-1-implementation/</guid>
      <description>



&lt;style&gt;
.math {
  font-size: small;
}
&lt;/style&gt;
&lt;style&gt;
&lt;p&gt;.column-left{
float: left;
width: 52%;
text-align: left;&lt;/p&gt;
&lt;p&gt;}&lt;/p&gt;
&lt;style&gt;
.column-center{
  float: center;
  width: 100%;
  text-align: left;

}

.column-right{
  float: right;
  width: 48%;
  text-align: left;
  margin-top: 6px;
  line-height: 1.83;
  font-size: 12px;

}
&lt;/style&gt;
&lt;style type=&#34;text/css&#34;&gt;
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 13.2px
}
&lt;/style&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The model presented herein uses modified code from &lt;a href=&#34;https://khayatrayen.github.io/MCMC.html&#34; class=&#34;uri&#34;&gt;https://khayatrayen.github.io/MCMC.html&lt;/a&gt;. I am currently developing a Metropolis-within-Gibbs algorithm for stratigraphic correlation of &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;&lt;sup&gt;13&lt;/sup&gt;C records with Andrew R. Millard and Martin R. Smith at the &lt;a href=&#34;https://smithlabdurham.github.io/#!team&#34;&gt;Smith Lab at Durham University&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Markov chain Monte Carlo (MCMC) methods are widely used to obtain posterior probabilities for the unknown parameters of Bayesian models. The &lt;a href=&#34;https://arxiv.org/pdf/1504.01896.pdf&#34;&gt;Metropolis algorithm&lt;/a&gt; builds a Markov chain for each parameter, which resembles the posterior distribution. This works by selecting arbitrary starting values for the parameters and calculating the resulting joint posterior probability. Then, new values for the parameters are randomly proposed, and the joint posterior probability is calculated with the new parameter values. If the posterior obtained with the new values is higher than that of the current values, the new values will be recorded and added to the Markov chains. Otherwise, the new value will be accepted with a probability equal to the ratio of the two posterior probabilities. If the proposed values result in a much lower posterior probability than the current values, the proposal will most likely be rejected. This process is repeated many times, and the resulting Markov chains converge on the posterior distributions of the parameters. The Metropolis algorithm requires symmetric proposal distributions and is a special case of the Metropolis-Hastings algorithm.&lt;/p&gt;
&lt;p&gt;To illustrate the implementation of the Metropolis algorithm, we turn to climatology: Latitudinal temperature gradients from Earth history are difficult to reconstruct due to the &lt;a href=&#34;https://www.lewisajones.com/post/uneven-spatial-sampling-and-reconstructing-global-palaeotemperatures/&#34;&gt;sparse and geographically variable sampling of proxy data in most geological intervals&lt;/a&gt;. To reconstruct plausible temperature gradients from a fragmentary proxy record, classical solutions like LOESS or standard generalised additive models are not optimal, as earth scientists have additional information on past temperature gradients that those models do not incorporate. Instead, I propose the use of a generalised logistic function (a modified &lt;a href=&#34;https://www.jstor.org/stable/23686557?seq=1#metadata_info_tab_contents&#34;&gt;Richard’s curve&lt;/a&gt;) that can readily incorporate information in addition to the proxy data. For example, we can instruct the model to force temperature to continuously decrease from the tropics toward the poles.&lt;/p&gt;
&lt;p&gt;To keep with the familiar notation in regression models, we set denote latitude as &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and temperature as &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Temperature is modelled as a function of latitude as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned} \begin{equation} \begin{array}{l} y_i \sim N(\mu_i, \sigma), ~~\\ \mu_i = A~+max(K-A,0)/(e^{Q(x_i-M)}), ~~~~~ i = 1,...,n. \end{array} \end{equation} \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the lower asymptote, &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the upper asymptote, &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; is the inflection point, i.e. the steepest point of the curve, and &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; controls the steepness of the curve. The difference &lt;span class=&#34;math inline&#34;&gt;\(K-A\)&lt;/span&gt; is constrained to be &lt;span class=&#34;math inline&#34;&gt;\(\ge 0\)&lt;/span&gt; to preclude inverse temperature gradients.&lt;/p&gt;
&lt;p&gt;In R code, we turn this into a function named &lt;span class=&#34;math inline&#34;&gt;\(gradient\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gradient &amp;lt;- function(x, coeff, sdy) { # sigma is labelled &amp;quot;sdy&amp;quot;
  A = coeff[1]
  K = coeff[2]
  M = coeff[3]
  Q = coeff[4]
  return(A + max(c(K-A,0))/((1+(exp(Q*(x-M))))) + rnorm(length(x),0,sdy))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, let’s look at the modern, average latitudinal sea surface temperature gradient. We approximate it by setting &lt;span class=&#34;math inline&#34;&gt;\(A = -2.0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(K = 28\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(M = 41\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Q = 0.10\)&lt;/span&gt;. The residual standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is set to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, resulting in a smooth curve without noise (lefthand plot). Note that we are using absolute latitudes, assuming a common latitudinal temperature gradient in both hemispheres.
We also sample &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; points from this gradient, introducing some noise by setting &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 2\)&lt;/span&gt; (righthand plot). In the following, we will use these &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; points to estimate a latitudinal gradient, using the gradient model specified above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(10)
sample_lat &amp;lt;- runif(10,0,90)
sample_data &amp;lt;- data.frame(
  x = sample_lat, 
  y = gradient(x = sample_lat, coeff = c(-2.0, 28, 41, 0.1), sd = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-metropolis-algorithm-in-r-part-1-implementation/index.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Before writing the main Markov chain Monte Carlo (MCMC) function, we pre-define a couple of supplementary functions that we use in every iteration of the Metropolis algorithm.&lt;/p&gt;
&lt;p&gt;We start with the log-likelihood function, which translates to the joint probability of the data, given a specific set of model parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loglik &amp;lt;- function(x, y,  coeff, sdy) {
  A = coeff[1]
  K = coeff[2]
  M = coeff[3]
  Q = coeff[4]
  pred = A + max(c(K-A,0))/((1+(exp(Q*(x-M)))))
  return(sum(dnorm(y, mean = pred, sd = sdy, log = TRUE)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need a function to generate the log-priors, i.e. the joint prior probability of the set of model parameters. We specify the parameters of the prior distribution within the function for convenience. Uniform priors ranging from &lt;span class=&#34;math inline&#34;&gt;\(-4\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(40\)&lt;/span&gt; are put on &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;, signifying that the temperature gradient cannot exceed this range. A normal prior with a mean of &lt;span class=&#34;math inline&#34;&gt;\(45\)&lt;/span&gt; and a standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; is placed on &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, implying that we expect the steepest temperature gradient in the mid-latitudes. We constrain &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; to be &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;0\)&lt;/span&gt; by placing a log-normal prior on it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logprior &amp;lt;- function(coeff) {
  return(sum(c(
    dunif(coeff[1], -4, 40, log = TRUE),
    dunif(coeff[2], -4, 40, log = TRUE),
    dnorm(coeff[3], 45, 10, log = TRUE),
    dlnorm(coeff[4], -2, 1, log = TRUE))))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The posterior is proportional to the likelihood &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; prior. On the log scale, we can simply add them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logposterior &amp;lt;- function(x, y, coeff, sdy){
  return (loglik(x, y, coeff, sdy) + logprior(coeff))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we define a function that proposes new values for the Metropolis-Hastings step. The magnitude of the proposal standard deviations (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt;) is quite important, as low values will lead to the chain exploring the parameter space very slowly, and high values result in a low acceptance rate and an insufficient exploration of the parameter space. As appropriate &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; are difficult to know &lt;em&gt;a priori&lt;/em&gt;, adaptive steps are often used to find better values. For simplicity, we will use fixed &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt;. Different &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt; can and usually should be used for different parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MH_propose &amp;lt;- function(coeff, proposal_sd){
  return(rnorm(4,mean = coeff, sd= c(.5,.5,.5,0.01)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With all the prerequisites in place, we can build the MCMC function. The model will update &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; with a Gibbs step, and update the other coefficients with a Metropolis-Hastings step:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;run_MCMC &amp;lt;- function(x, y, coeff_inits, sdy_init, nIter){
  ### Initialisation
  coefficients = array(dim = c(nIter,4)) # set up array to store coefficients
  coefficients[1,] = coeff_inits # initialise coefficients
  sdy = rep(NA_real_,nIter) # set up vector to store sdy
  sdy[1] = sdy_init # intialise sdy
  A_sdy = 3 # parameter for the prior on the inverse gamma distribution of sdy
  B_sdy = 0.1 # parameter for the prior on the inverse gamma distribution of sdy
  n &amp;lt;- length(y)
  shape_sdy &amp;lt;- A_sdy+n/2 # shape parameter for the inverse gamma
  
  ### The MCMC loop
  for (i in 2:nIter){ 
    
    ## 1. Gibbs step to estimate sdy
    sdy[i] = sqrt(1/rgamma(
      1,shape_sdy,B_sdy+0.5*sum((y-gradient(x,coefficients[i-1,],0))^2)))
    
    ## 2. Metropolis-Hastings step to estimate the regression coefficients
    proposal = MH_propose(coefficients[i-1,]) # new proposed values
    if(any(proposal[4] &amp;lt;= 0)) HR = 0 else # Q needs to be &amp;gt;0
    # Hastings ratio of the proposal
    HR = exp(logposterior(x = x, y = y, coeff = proposal, sdy = sdy[i]) -
             logposterior(x = x, y = y, coeff = coefficients[i-1,], sdy = sdy[i]))
    # accept proposal with probability = min(HR,1)
    if (runif(1) &amp;lt; HR){ 
      coefficients[i,] = proposal
    # if proposal is rejected, keep the values from the previous iteration
    }else{
      coefficients[i,] = coefficients[i-1,]
    }
  } # end of the MCMC loop
  
  ###  Function output
  output = data.frame(A = coefficients[,1],
                      K = coefficients[,2],
                      M = coefficients[,3],
                      Q = coefficients[,4],
                      sdy = sdy)
  return(output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run the model, we need to provide starting values for the unknown parameters. We let it run for &lt;span class=&#34;math inline&#34;&gt;\(100,000\)&lt;/span&gt; iterations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nIter &amp;lt;- 100000
m &amp;lt;- run_MCMC(x = sample_data$x, y = sample_data$y, 
              coeff_inits = c(0,30,45,0.2), sdy_init = 4, nIter = nIter)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To assess the model output, we produce trace plots and density plots of the posterior estimates of the parameters. For the trace plot, only every &lt;span class=&#34;math inline&#34;&gt;\(10^{th}\)&lt;/span&gt; iteration is shown to improve readability. The black lines in the density plot denote the parameters of the original sea surface temperature gradient.
&lt;img src=&#34;/post/a-metropolis-algorithm-in-r-part-1-implementation/index.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;691.2&#34; /&gt;
The parameters have converged reasonably well.&lt;/p&gt;
&lt;p&gt;Below, we discard the first &lt;span class=&#34;math inline&#34;&gt;\(10,000\)&lt;/span&gt; iterations as burn-in and plot &lt;span class=&#34;math inline&#34;&gt;\(8\)&lt;/span&gt; gradients, using different samples from the posterior (blue lines, lefthand plot). As expected, they fit nicely to the &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; sampled data points (grey dots). They are also quite similar to the original gradient (black, dashed line). To the right, the estimated temperature gradient using the median of the parameters from the posterior (blue line), and &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; credible intervals (blue shading), are shown. Between &lt;span class=&#34;math inline&#34;&gt;\(10^\circ\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(50^\circ\)&lt;/span&gt;, where we have sufficient samples, the estimated gradient very closely resembles the original gradient. The constraints imposed by the priors ensure that the estimated sea surface temperature gradients stays in a realistic range (&lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;-4^\circ C\)&lt;/span&gt;), even at latitudes &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt; 63^\circ\)&lt;/span&gt; where we have no data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-metropolis-algorithm-in-r-part-1-implementation/index.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;864&#34; /&gt;
In conclusion, the model seems to be doing a good job in estimating a sensible temperature gradient from sparse samples. In the &lt;a href=&#34;/post/a-metropolis-algorithm-in-r-part-2-adaptive-proposals&#34;&gt;second part&lt;/a&gt; of this series, we will implement adaption of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt;, which means we won’t have to guess good values for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{proposal}\)&lt;/span&gt;. This should speed up convergence, meaning we will need less iterations of the MCMC algorithm to obtain reliable posterior estimates.&lt;/p&gt;
&lt;p&gt;You can find the full R code to reproduce all analyses and figures on &lt;a href=&#34;https://github.com/KEichenseer/Methods/blob/main/A_Metropolis_algorithm_in_R_generalised_logistic_function.R&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Change Point Regression</title>
      <link>/post/change-point-regression/</link>
      <pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/change-point-regression/</guid>
      <description>
&lt;script src=&#34;/post/change-point-regression/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;style&gt;
&lt;p&gt;.column-left{
float: left;
width: 52%;
text-align: left;&lt;/p&gt;
&lt;p&gt;}&lt;/p&gt;
&lt;style&gt;
.column-center{
  float: center;
  width: 100%;
  text-align: left;

}

.column-right{
  float: right;
  width: 48%;
  text-align: left;
  margin-top: 6px;
  line-height: 1.83;
  font-size: 12px;

}
&lt;/style&gt;
&lt;style type=&#34;text/css&#34;&gt;
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 13.2px
}
&lt;/style&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;This implementation of change point regression was developed by &lt;a href=&#34;https://www.plymouth.ac.uk/staff/julian-stander&#34;&gt;Julian Stander&lt;/a&gt; (University of Plymouth) in &lt;a href=&#34;https://www.nature.com/articles/s41561-019-0392-9&#34;&gt;Eichenseer et al. (2019)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Assume we want to investigate the relationship between two variables, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, that we have collected over a certain period of time. We have reason to believe that the relationship changed at some point, but we don’t know when.&lt;/p&gt;
&lt;p&gt;Let’s generate &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and plot them. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is linearly dependent on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; across the whole time series, but we induce an increase in the intercept, slope and residual variance at the &lt;span class=&#34;math inline&#34;&gt;\(35^{th}\)&lt;/span&gt; observation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(10) # change the seed for a different sequence of random numbers
n &amp;lt;- 60 # number of total data points
n_shift &amp;lt;- 35 # the data point at which we introduce a change
x &amp;lt;- rnorm(n,0,1) # generate x
y &amp;lt;- rnorm(n,0,0.5) + 0.5 * x # generate y without a change
y[n_shift:n] &amp;lt;- rnorm(length(n_shift:n),0,1) + 1 * x[n_shift:n] + 0.75 # introduce change&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/change-point-regression/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;the-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The regression model&lt;/h2&gt;
&lt;p&gt;Now we build a model that can recover the change point and the linear relationship between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; before and after the change point.&lt;/p&gt;
&lt;p&gt;The first part of this model looks like an ordinary least squares regression of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; against &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned} \begin{equation} \begin{array}{l} y_i \sim N(\mu_i, \sigma_1^2), ~~\\ \mu_i = \alpha_1~+~\beta_1~x_i, ~~~~~ i = 1,...,n_{change}-1 \end{array} \end{equation} \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we have a single intercept (&lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt;), slope (&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;), and residual variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_1\)&lt;/span&gt;). &lt;span class=&#34;math inline&#34;&gt;\(n_{change}\)&lt;/span&gt; - 1 denotes the number of obervations before the change point.&lt;/p&gt;
&lt;p&gt;From the change point &lt;span class=&#34;math inline&#34;&gt;\(n_{change}\)&lt;/span&gt; onwards, we add an additional intercept, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_2\)&lt;/span&gt;, to the intercept from the first part (&lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt;). We do the same for the slope and the residual variance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned} \begin{equation} \begin{array}{l} y_i \sim N(\mu_i, \sigma_1^2+\sigma_2^2), ~~\\ \mu_i = \alpha_1~+~\alpha_2~+~(\beta_1~+~\beta_2)~x_i, ~~~~~ i = n_{change},...,n \end{array} \end{equation} \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; denotes the total number of observations, 60 in this case. But how do we actually find the change point &lt;span class=&#34;math inline&#34;&gt;\(n_{change}\)&lt;/span&gt;?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-in-jags&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementation in JAGS&lt;/h2&gt;
&lt;p&gt;Here, we turn to the &lt;a href=&#34;https://mcmc-jags.sourceforge.io/&#34;&gt;JAGS programming environment&lt;/a&gt;. Understanding a model written for JAGS is not easy at first. If you are keen on learning Bayesian modeling from scratch I can highly recommend Richard McElreath’s book &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;Statistical Rethinking&lt;/a&gt;. We will access JAGS with the &lt;a href=&#34;https://CRAN.R-project.org/package=R2jags&#34;&gt;R2jags package&lt;/a&gt;, so we can keep using R even if we are writing a model for JAGS.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Bayesian methods for detecting change points are also available in &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;, as discussed &lt;a href=&#34;https://mc-stan.org/docs/2_27/stan-users-guide/change-point-section.html&#34;&gt;here&lt;/a&gt;. An application using English league football data can be found &lt;a href=&#34;https://www.significancemagazine.com/sports/693-has-english-league-football-become-less-exciting&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Below, we look at the model. The R code that will be passed to JAGS later is on the left. On the right is an explanation for each line of the model.&lt;/p&gt;
&lt;div class=&#34;column-right&#34;&gt;
&lt;p&gt;We save the model as a function named&lt;br /&gt;
&lt;em&gt;model_CPR&lt;/em&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Loop over all the data points &lt;span class=&#34;math inline&#34;&gt;\(1,...,n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_i \sim N(\mu_i, \tau_i)\)&lt;/span&gt;&lt;br /&gt;
note that JAGS uses the precision &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; instead&lt;br /&gt;
of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.     &lt;span class=&#34;math inline&#34;&gt;\(\tau = 1/\sigma^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;step&lt;/em&gt; takes the value &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; if its argument is &lt;span class=&#34;math inline&#34;&gt;\(\ge 0\)&lt;/span&gt;,&lt;br /&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; otherwise, resulting in&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mu_i = \alpha_1~+~\beta_1~x_i\)&lt;/span&gt;     before &lt;span class=&#34;math inline&#34;&gt;\(n_{change}\)&lt;/span&gt; and&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mu_i = \alpha_1~+~\alpha_2~+~(\beta_1~+~\beta_2)~x_i\)&lt;/span&gt;&lt;br /&gt;
from &lt;span class=&#34;math inline&#34;&gt;\(n_{change}\)&lt;/span&gt;   onwards.&lt;/p&gt;
&lt;p&gt;back-transform &lt;span class=&#34;math inline&#34;&gt;\(\log(\tau)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;again, the &lt;em&gt;step&lt;/em&gt; function is used to define &lt;span class=&#34;math inline&#34;&gt;\(\log(\tau)\)&lt;/span&gt; before and after &lt;span class=&#34;math inline&#34;&gt;\(n_{change}\)&lt;/span&gt;. Log-transformation is used to ensure that the &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; resulting from &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau_2\)&lt;/span&gt; is positive.&lt;/p&gt;
&lt;p&gt;We have to define priors for all parameters that are not specified by data.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_1 \sim N(\mu = 0, \tau = 10^{-4})\)&lt;/span&gt; That is a normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu = 0\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 100\)&lt;/span&gt;,&lt;br /&gt;
because &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 1/\sqrt{\tau}\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\alpha_2 \sim N(0, 10^{-4})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1 \sim N(0, 10^{-4})\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_2 \sim N(0, 10^{-4})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\log(\tau_1) \sim N(0, 10^{-4})\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\log(\tau_2) \sim N(0, 10^{-4})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Discrete prior on the change point. &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; indicates one of the possible change points,
based on the probability vector &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which we need to specify beforehand.&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_CPR &amp;lt;- function(){
  
  ### Likelihood or data model part
  for(i in 1:n){
    
  y[i] ~ dnorm(mu[i], tau[i]) 

    
    
  mu[i] &amp;lt;- alpha_1 + 
  alpha_2 * step(i - n_change) +
  (beta_1 + beta_2 * step(i - n_change))*x[i]
  
  
  
  tau[i] &amp;lt;- exp(log_tau[i])
  
  log_tau[i] &amp;lt;- log_tau_1 + log_tau_2 * 
  step(i - n_change)
  } 
  
  ### Priors
  
  
  alpha_1 ~ dnorm(0, 1.0E-4)
  
  
  alpha_2 ~ dnorm(0, 1.0E-4)
  
  beta_1 ~ dnorm(0, 1.0E-4)
  beta_2 ~ dnorm(0, 1.0E-4)
  
  log_tau_1 ~ dnorm(0, 1.0E-4)
  log_tau_2 ~ dnorm(0, 1.0E-4)
  
  K ~ dcat(p)
  n_change &amp;lt;- possible_change_points[K]

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we put priors on &lt;span class=&#34;math inline&#34;&gt;\(\log(\tau_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\log(\tau_2)\)&lt;/span&gt;, rather than on &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau_2\)&lt;/span&gt; directly, to ensure that the precision &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; in the second part of the regression always remains positive. &lt;span class=&#34;math inline&#34;&gt;\(e^{\log(\tau_1) + \log(\tau_2)}\)&lt;/span&gt; is always &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt; 0\)&lt;/span&gt;, even if the term &lt;span class=&#34;math inline&#34;&gt;\(\log(\tau_1)\)&lt;/span&gt; + &lt;span class=&#34;math inline&#34;&gt;\(\log(\tau_2)\)&lt;/span&gt; becomes negative.&lt;/p&gt;
&lt;p&gt;Prepare the data which we pass to JAGS along with the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# minimum number of the data points before and after the change
  min_segment_length &amp;lt;- 5 

# assign indices to the potential change points we allow
  possible_change_points &amp;lt;- (1:n)[(min_segment_length+1):(n+1-min_segment_length)] 
 
# number of possible change points
  M &amp;lt;- length(possible_change_points)  

# probabilities for the discrete uniform prior on the possible change points, 
# i.e. all possible change points have the same prior probability
  p &amp;lt;- rep(1 / M, length = M) 
 
# save the data to a list for jags
  data_CPR &amp;lt;- list(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;possible_change_points&amp;quot;, &amp;quot;p&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the &lt;em&gt;R2jags&lt;/em&gt; package to access &lt;em&gt;JAGS&lt;/em&gt; in &lt;em&gt;R&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  library(R2jags) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we execute the change point regression. We instruct JAGS to run three seperate chains so we can verify that the results are consistent. We allow 2000 iterations of the Markov chain Monte Carlo algorithm for each chain, the first 1000 of which will automatically be discarded as burn-in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; CPR  &amp;lt;- jags(data = data_CPR, 
                         parameters.to.save = c(&amp;quot;alpha_1&amp;quot;, &amp;quot;alpha_2&amp;quot;, 
                                                &amp;quot;beta_1&amp;quot;,&amp;quot;beta_2&amp;quot;,
                                                &amp;quot;log_tau_1&amp;quot;,&amp;quot;log_tau_2&amp;quot;,
                                                &amp;quot;n_change&amp;quot;), 
                         n.iter = 2000, 
                         n.chains = 3,
                         model.file = model_CPR)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The results&lt;/h2&gt;
&lt;p&gt;To visualise the results and inspect the posterior, we are using the &lt;em&gt;ggmcmc&lt;/em&gt; package, which relies on the &lt;em&gt;ggplot2&lt;/em&gt; package. For brevity, we just look at the &lt;span class=&#34;math inline&#34;&gt;\(n_{change}\)&lt;/span&gt; parameter here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmcmc)
CPR.ggs &amp;lt;- ggs(as.mcmc(CPR)) # convert to ggs object
ggs_traceplot(CPR.ggs, family = &amp;quot;n_change&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/change-point-regression/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1000 %&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like the chains converge and mix nicely. We can already see that our model locates the change point somewhere between &lt;span class=&#34;math inline&#34;&gt;\(30\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(40\)&lt;/span&gt;, although the chains occasionally explore regions further away.&lt;/p&gt;
&lt;p&gt;Let’s look at the posterior probabilities for the possible change points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = CPR.ggs %&amp;gt;% filter(Parameter == &amp;quot;n_change&amp;quot;),
  aes(x=value, y = 3*(..count..)/sum(..count..), fill = as.factor(Chain))) + 
  geom_vline(xintercept = 35,lty = 2) + geom_bar(position = &amp;quot;identity&amp;quot;, alpha = 0.5) +
  ylab(&amp;quot;posterior probability&amp;quot;) + xlab(&amp;quot;n_change&amp;quot;) + labs(fill=&amp;#39;Chain&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/change-point-regression/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;700 %&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(37^{th}\)&lt;/span&gt; point has the highest probability of being the change point. That is not far off from where we introduced the change, at the &lt;span class=&#34;math inline&#34;&gt;\(35^{th}\)&lt;/span&gt; point (dashed line). The random generation of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; has led to &lt;span class=&#34;math inline&#34;&gt;\(37\)&lt;/span&gt; being favoured. We also note that there are only minor differences between the three chains, and those differences would likely further dwindle if we were to let the chains run for longer.&lt;/p&gt;
&lt;p&gt;Using the posterior distribution, we can answer questions like: “In which interval does the change point fall with 90 % probability?”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(CPR$BUGSoutput$sims.list$n_change, probs = c(0.05, 0.95))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  5% 95% 
##  33  39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also inquire about the probability that the change point falls in the interval &lt;span class=&#34;math inline&#34;&gt;\(34\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(38\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(length(which(CPR$BUGSoutput$sims.list$n_change %in% 34:38))/
              (CPR$BUGSoutput$n.sims),2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.87&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let’s have a look at the regression parameters and plot the resulting regressions before and after the most likely change point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/change-point-regression/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;475 %&#34; /&gt;
The intercept, slope, and residual variance all increase after the change point.&lt;/p&gt;
&lt;p&gt;This can be immediately seen when plotting the change point regression:
&lt;img src=&#34;/post/change-point-regression/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;375 %&#34; /&gt;
The shaded areas denote &lt;span class=&#34;math inline&#34;&gt;\(95\)&lt;/span&gt; % credible intervals around the regression lines.&lt;/p&gt;
&lt;p&gt;You can find the full R code for this analysis at &lt;a href=&#34;https://github.com/KEichenseer/Methods/blob/main/Change_point_regression.R&#34; class=&#34;uri&#34;&gt;https://github.com/KEichenseer/Methods/blob/main/Change_point_regression.R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Get in touch if you have any comments or questions!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
