---
title: Bayesian errors-in-variables regression with Gibbs sampling in R
author: ''
date: '2023-04-02'
slug: bayesian-errors-in-variables-regression-with-gibbs-sampling-in-r
categories: [R, Bayesian, JAGS, Gibbs sampling]
tags: []
subtitle: ''
authors: []
#featured: false
draft: true
image:
  caption: ''
  focal_point: ''
  preview_only: true
projects: []
header-includes: \usepackage{graphics}
summary: Errors-in-variables regression is preferable to simple linear regression when the observations come with uncertainty. Here, I first show how errors-in-variables regression can be done in R, using JAGS. I then construct a Gibbs sampler that runs purely in R, without any external software like JAGS or Stan. 

---




<style>
.math {
  font-size: small;
}
</style>
<style>
<p>.column-left{
float: left;
width: 52%;
text-align: left;</p>
<p>}</p>
<style>
.column-center{
  float: center;
  width: 100%;
  text-align: left;

}

.column-right{
  float: right;
  width: 48%;
  text-align: left;
  margin-top: 6px;
  line-height: 1.83;
  font-size: 12px;

}
</style>
<style type="text/css">
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 13.2px
}
</style>
<p>Assume we want to conduct a linear regression of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>. Both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> come with normally distributed uncertainties or measurement errors, which we need to take into consideration, or we risk obtaining biased regression coefficients. Let’s generate <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, with uncertainties denoted as standard deviations <span class="math inline">\(sigma_x\)</span> and <span class="math inline">\(sigma_y\)</span>. Each point may have a different uncertainty. Below, we generate some sample data.</p>
<pre class="r"><code>set.seed(1) # change the seed for a different sequence of random numbers
n &lt;- 30 # number of total data points
x &lt;- runif(n,-2,2) # generate the true x
y &lt;- 2 + 0.75 * x # generate the true y
# define the standard deviations of the normal uncertainties with which x and y where observed
x_sd &lt;- runif(n,0.2,0.5)
y_sd &lt;- runif(n,0.2,1)
# generate observations from x and y, given these uncertainties
x_obs &lt;- rnorm(n,x,x_sd)
y_obs &lt;- rnorm(n,y,y_sd)</code></pre>
<p>Now, we show the observations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, along with the true underlying relationship. We also add a simple linear regression, using the R functions <code>lm()</code> and `predict.lm(), to compare it to the true relationship:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="384" /></p>
<p>Next, we build a Bayesian, linear regression model using JAGS, that accounts for
the uncertainties around our observations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. As we can almost directly
pass the model formulas to JAGS using the <code>R2jags</code> package, this is quite straightforward.
Here is the JAGS model:</p>
<pre class="r"><code>errors_in_variables_regression_jags &lt;- function() {
  ## Likelihood
  for (i in 1:n){ # 
    x_est[i] ~ dnorm(x_obs[i], 1/(x_sd[i]*x_sd[i])) # precision = 1/sigma^2
    y_obs[i] ~ dnorm(y_est[i], 1/(y_sd[i]*y_sd[i])) # precision = 1/sigma^2
    y_est[i] ~ dnorm(mu[i], tau) # JAGS uses precision `tau` instead of sigma
    mu[i] &lt;- alpha + beta * x_est[i]
  }
  ## Priors
  tau ~ dgamma(1, 1)  # gamma prior for precision
  sigma &lt;- 1/sqrt(tau) # calculate standard deviation
  alpha ~ dnorm(0, 1/(100*100))
  beta ~ dnorm(0, 1/(100*100))
}</code></pre>
<p>We load the <code>R2jags</code> package, specify the data used in the model, and send everything to JAGS:</p>
<pre class="r"><code>library(R2jags)

regression_data &lt;- list(&quot;x_obs&quot;, &quot;y_obs&quot;, &quot;x_sd&quot;, &quot;y_sd&quot;,&quot;n&quot;)

lm_jags  &lt;- jags(data = regression_data,
                parameters.to.save = c(&quot;alpha&quot;,
                                       &quot;beta&quot;,
                                       &quot;sigma&quot;,
                                       &quot;y_est&quot;,
                                       &quot;x_est&quot;
                ),
                n.iter = 3000,
                n.thin = 1,
                n.chains =  3, # Other values set at default (for simplicity)
                model.file = errors_in_variables_regression_jags)</code></pre>
<p>Let’s visualise this regression and compare it to the simple linear regression from above:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="384" /></p>
<pre><code>## [[1]]
## NULL
## 
## [[2]]
## NULL
## 
## [[3]]
## NULL
## 
## [[4]]
## NULL
## 
## [[5]]
## NULL
## 
## [[6]]
## NULL
## 
## [[7]]
## NULL
## 
## [[8]]
## NULL
## 
## [[9]]
## NULL
## 
## [[10]]
## NULL
## 
## [[11]]
## NULL
## 
## [[12]]
## NULL
## 
## [[13]]
## NULL
## 
## [[14]]
## NULL
## 
## [[15]]
## NULL
## 
## [[16]]
## NULL
## 
## [[17]]
## NULL
## 
## [[18]]
## NULL
## 
## [[19]]
## NULL
## 
## [[20]]
## NULL
## 
## [[21]]
## NULL
## 
## [[22]]
## NULL
## 
## [[23]]
## NULL
## 
## [[24]]
## NULL
## 
## [[25]]
## NULL
## 
## [[26]]
## NULL
## 
## [[27]]
## NULL
## 
## [[28]]
## NULL
## 
## [[29]]
## NULL
## 
## [[30]]
## NULL</code></pre>
<p>The mean regression line is very similar to the simple linear regression, but the uncertainty is now larger. Also, notice how the estimates of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (orange triangles) have been shifted towards the mean regression line - this effect is called shrinkage, and is characteristic of hierarchical models.</p>
