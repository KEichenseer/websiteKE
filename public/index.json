[{"authors":null,"categories":null,"content":"Centuries of palaeontological and geological research have produced vast quantities of information on past organisms and environments. Using statistical tools, I am probing these data riches to reconstruct novel aspects of the history of Life. Focal points of my research have been coral reefs, palaeoclimate and the skeletal mineralogy of marine invertebrates. I am currently building a Bayesian stratigraphic model to correlate Cambrian sections using $\\delta$13C records. (https://smithlabdurham.github.io/#!team).\n\r Download my CV.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"55a3e4280500323cdcae79b533c07301","permalink":"/author/kilian-eichenseer-phd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kilian-eichenseer-phd/","section":"authors","summary":"Centuries of palaeontological and geological research have produced vast quantities of information on past organisms and environments. Using statistical tools, I am probing these data riches to reconstruct novel aspects of the history of Life.","tags":null,"title":"Kilian Eichenseer PhD","type":"authors"},{"authors":null,"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e87efdfe209d90ea3c6332a7cbd9d08b","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Âê≥ÊÅ©ÈÅî","type":"authors"},{"authors":null,"categories":null,"content":"\r\r Table of Contents\r What you will learn Program overview Courses in this program Meet your instructor FAQs  \r\rWhat you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program \rPython basics\rBuild a foundation in Python. \r\rVisualization\rLearn how to visualize data with Plotly. \r\rStatistics\rIntroduction to statistics for data science. \r\rMeet your instructor admin FAQs Are there prerequisites?\rThere are no prerequisites for the first course.\n How often do the courses run?\rContinuously, at your own pace.\n \rBegin the course\r\r\r","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n\r 1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples?\rLists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive?\rYes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n\r 1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful?\rLorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart\rimport plotly.express as px\rdata_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;)\rfig = px.bar(data_canada, x='year', y='pop')\rfig.show()\r ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n\r 1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n\rThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.\r\r\rQuiz What is the parameter $\\mu$?\rThe parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":"\rClick on the Slides button above to view the built-in slides feature.\r\r\rSlides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":[],"categories":[],"content":"\r\r.math {\rfont-size: small;\r}\r\rbody, td {\rfont-size: 14px;\r}\rcode.r{\rfont-size: 12px;\r}\rpre {\rfont-size: 13.2px\r}\r\r\rThe model presented herein uses modified code from https://khayatrayen.github.io/MCMC.html. I am currently developing a Metropolis-within-Gibbs algorithm for stratigraphic correlation of \\(\\delta\\)13C records with Andrew R. Millard and Martin R. Smith at the Smith Lab at Durham University.\n\rIn the previous post, we built a Metropolis algorithm to estimate latitudinal temperature gradients, approximated by a generalised logistic function. Recall that the Metropolis algorithm works by proposing new parameter values and evaluating the joint posterior probability of the model with these values, against the posterior with the current values.\nHow do we chose a new value for a parameter? A common approach is to sample a normal distribution, centred at the current value (i.e.¬†the mean of the distribution is the current value). Choosing the standard deviation of the proposal distribution (\\(\\sigma_{proposal}\\)) is more tricky. If \\(\\sigma_{proposal}\\) is too high, we end up proposing a lot of values at the far tail ends of the target posterior distribution, which will usually be rejected (see below, green proposals). This leads to inefficient sampling and patchy coverage of the posterior distribution. Conversely, a very small \\(\\sigma_{proposal}\\) leads to most new values being accepted, but the resulting Markov chain will move very slowly through the parameter space, leading to a low effective sample size (red proposals below). Instead, some intermediate \\(\\sigma_{proposal}\\) is desirable, at which the Markov chain moves quickly through the parameter space, without too many rejections (e.g., yellow proposals below).\nIt turns out that, for multidimensional problems, sampling is most efficient if the acceptance rate of proposals is roughly \\(1/4\\) (Gelman et al.¬†1997).\ntest, we developed a Metropolis algorithm to construct a latitudinal temperature gradient from a small sample of temperatures values.\n","date":1648252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648325584,"objectID":"8a7f62ae516c5ac4016dc622f9ad94e9","permalink":"/post/a-metropolis-algorithm-in-r-part-2-adaptive-proposals/","publishdate":"2022-03-26T00:00:00Z","relpermalink":"/post/a-metropolis-algorithm-in-r-part-2-adaptive-proposals/","section":"post","summary":"The Metropolis algorithm is a simple, but powerful MCMC method. Here, I use it for estimating a generalised logistic function to reconstruct a latitudinal climate gradient from a small sample of temperature estimates.","tags":[],"title":"A Metropolis algorithm in R - Part 2: Adaptive proposals","type":"post"},{"authors":[],"categories":[],"content":"\r\r.math {\rfont-size: small;\r}\r\r\r.column-left{\rfloat: left;\rwidth: 52%;\rtext-align: left;\n}\n\r.column-center{\rfloat: center;\rwidth: 100%;\rtext-align: left;\r}\r.column-right{\rfloat: right;\rwidth: 48%;\rtext-align: left;\rmargin-top: 6px;\rline-height: 1.83;\rfont-size: 12px;\r}\r\rbody, td {\rfont-size: 14px;\r}\rcode.r{\rfont-size: 12px;\r}\rpre {\rfont-size: 13.2px\r}\r\r\rThe model presented herein uses modified code from https://khayatrayen.github.io/MCMC.html. I am currently developing a Metropolis-within-Gibbs algorithm for stratigraphic correlation of \\(\\delta\\)13C records with Andrew R. Millard and Martin R. Smith at the Smith Lab at Durham University.\n\rMarkov chain Monte Carlo (MCMC) methods are widely used to obtain posterior probabilities for the unknown parameters of Bayesian models. The Metropolis algorithm builds a Markov chain for each parameter, which resembles the posterior distribution. This works by selecting arbitrary starting values for the parameters and calculating the resulting joint posterior probability. Then, new values for the parameters are randomly proposed, and the joint posterior probability is calculated with the new parameter values. If the posterior obtained with the new values is higher than that of the current values, the new values will be recorded and added to the Markov chains. Otherwise, the new value will be accepted with a probability equal to the ratio of the two posterior probabilities. If the proposed values result in a much lower posterior probability than the current values, the proposal will most likely be rejected. This process is repeated many times, and the resulting Markov chains converge on the posterior distributions of the parameters. The Metropolis algorithm requires symmetric proposal distributions and is a special case of the Metropolis-Hastings algorithm.\nTo illustrate the implementation of the Metropolis algorithm, we turn to climatology: Latitudinal temperature gradients from Earth history are difficult to reconstruct due to the sparse and geographically variable sampling of proxy data in most geological intervals. To reconstruct plausible temperature gradients from a fragmentary proxy record, classical solutions like LOESS or standard generalised additive models are not optimal, as earth scientists have additional information on past temperature gradients that those models do not incorporate. Instead, I propose the use of a generalised logistic function (a modified Richard‚Äôs curve) that can readily incorporate information in addition to the proxy data. For example, we can instruct the model to force temperature to continuously decrease from the tropics toward the poles.\nTo keep with the familiar notation in regression models, we set denote latitude as \\(x\\) and temperature as \\(y\\). Temperature is modelled as a function of latitude as:\n\\(\\begin{aligned} \\begin{equation} \\begin{array}{l} y_i \\sim N(\\mu_i, \\sigma), ~~\\\\ \\mu_i = A~+max(K-A,0)/(e^{Q(x_i-M)}), ~~~~~ i = 1,...,n. \\end{array} \\end{equation} \\end{aligned}\\)\n\\(A\\) is the lower asymptote, \\(K\\) is the upper asymptote, \\(M\\) is the inflection point, i.e.¬†the steepest point of the curve, and \\(Q\\) controls the steepness of the curve. The difference \\(K-A\\) is constrained to be \\(\\ge 0\\) to preclude inverse temperature gradients.\nIn R code, we turn this into a function named \\(gradient\\):\ngradient \u0026lt;- function(x, coeff, sdy) { # sigma is labelled \u0026quot;sdy\u0026quot;\rA = coeff[1]\rK = coeff[2]\rM = coeff[3]\rQ = coeff[4]\rreturn(A + max(c(K-A,0))/((1+(exp(Q*(x-M))))) + rnorm(length(x),0,sdy))\r}\rAs an example, let‚Äôs look at the modern, average latitudinal sea surface temperature gradient. We approximate it by setting \\(A = -2.0\\), \\(K = 28\\), \\(M = 41\\), and \\(Q = 0.10\\). The residual standard deviation \\(\\sigma\\) is set to \\(0\\), resulting in a smooth curve without noise (lefthand plot). Note that we are using absolute latitudes, assuming a common latitudinal temperature gradient in both hemispheres.\rWe also sample \\(10\\) points from this gradient, introducing some noise by setting \\(\\sigma = 2\\) (righthand plot). In the following, we will use these \\(10\\) points to estimate a latitudinal gradient, using the gradient model specified above.\nset.seed(10)\rsample_lat \u0026lt;- runif(10,0,90)\rsample_data \u0026lt;- data.frame(\rx = sample_lat, y = gradient(x = sample_lat, coeff = c(-2.0, 28, 41, 0.1), sd = 2))\rBefore writing the main Markov chain Monte Carlo (MCMC) function, we pre-define a couple of supplementary functions that we use in every iteration of the Metropolis algorithm.\nWe start with the log-likelihood function, which translates to the joint probability of the data, given a specific set of model parameters:\nloglik \u0026lt;- function(x, y, coeff, sdy) {\rA = coeff[1]\rK = coeff[2]\rM = coeff[3]\rQ = coeff[4]\rpred = A + max(c(K-A,0))/((1+(exp(Q*(x-M)))))\rreturn(sum(dnorm(y, mean = pred, sd = sdy, log = TRUE)))\r}\rNext, we need a function to generate the log-priors, i.e.¬†the joint prior probability of the set of model parameters. We specify the parameters of the prior distribution within the function for convenience. Uniform priors ranging from \\(-4\\) to \\(40\\) are put on \\(A\\) and \\(K\\), signifying that the temperature gradient cannot exceed this range. A normal prior with a mean of \\(45\\) and a standard deviation of \\(10\\) is placed on \\(M\\), implying that we expect the steepest temperature gradient in the mid-latitudes. We constrain \\(Q\\) to be \\(\u0026gt;0\\) by placing a log-normal prior on it:\nlogprior \u0026lt;- function(coeff) {\rreturn(sum(c(\rdunif(coeff[1], -4, 40, log = TRUE),\rdunif(coeff[2], -4, 40, log = TRUE),\rdnorm(coeff[3], 45, 10, log = TRUE),\rdlnorm(coeff[4], -2, 1, log = TRUE))))\r}\rThe posterior is proportional to the likelihood \\(\\times\\) prior. On the log scale, we can simply add them:\nlogposterior \u0026lt;- function(x, y, coeff, sdy){\rreturn (loglik(x, y, coeff, sdy) + logprior(coeff))\r}\rFinally, we define a function that proposes new values for the Metropolis-Hastings step. The magnitude of the proposal standard deviations (\\(\\sigma_{proposal}\\)) is quite important, as low values will lead to the chain exploring the parameter space very slowly, and high values result in a low acceptance rate and an insufficient exploration of the parameter space. As appropriate \\(\\sigma_{proposal}\\) are difficult to know a priori, adaptive steps are often used to find better values. For simplicity, we will use fixed \\(\\sigma_{proposal}\\). Different \\(\\sigma_{proposal}\\) can and usually should be used for different parameters.\nMH_propose \u0026lt;- function(coeff, proposal_sd){\rreturn(rnorm(4,mean = coeff, sd= c(.5,.5,.5,0.01)))\r}\rWith all the prerequisites in place, we can build the MCMC function. The model will update \\(\\sigma\\) with a Gibbs step, and update the other coefficients with a Metropolis-Hastings step:\nrun_MCMC \u0026lt;- function(x, y, coeff_inits, sdy_init, nIter){\r### Initialisation\rcoefficients = array(dim = c(nIter,4)) # set up array to store coefficients\rcoefficients[1,] = coeff_inits # initialise coefficients\rsdy = rep(NA_real_,nIter) # set up vector to store sdy\rsdy[1] = sdy_init # intialise sdy\rA_sdy = 3 # parameter for the prior on the inverse gamma distribution of sdy\rB_sdy = 0.1 # parameter for the prior on the inverse gamma distribution of sdy\rn \u0026lt;- length(y)\rshape_sdy \u0026lt;- A_sdy+n/2 # shape parameter for the inverse gamma\r### The MCMC loop\rfor (i in 2:nIter){ ## 1. Gibbs step to estimate sdy\rsdy[i] = sqrt(1/rgamma(\r1,shape_sdy,B_sdy+0.5*sum((y-gradient(x,coefficients[i-1,],0))^2)))\r## 2. Metropolis-Hastings step to estimate the regression coefficients\rproposal = MH_propose(coefficients[i-1,]) # new proposed values\rif(any(proposal[4] \u0026lt;= 0)) HR = 0 else # Q needs to be \u0026gt;0\r# Hastings ratio of the proposal\rHR = exp(logposterior(x = x, y = y, coeff = proposal, sdy = sdy[i]) -\rlogposterior(x = x, y = y, coeff = coefficients[i-1,], sdy = sdy[i]))\r# accept proposal with probability = min(HR,1)\rif (runif(1) \u0026lt; HR){ coefficients[i,] = proposal\r# if proposal is rejected, keep the values from the previous iteration\r}else{\rcoefficients[i,] = coefficients[i-1,]\r}\r} # end of the MCMC loop\r### Function output\routput = data.frame(A = coefficients[,1],\rK = coefficients[,2],\rM = coefficients[,3],\rQ = coefficients[,4],\rsdy = sdy)\rreturn(output)\r}\rTo run the model, we need to provide starting values for the unknown parameters. We let it run for \\(100,000\\) iterations:\nnIter \u0026lt;- 100000\rm \u0026lt;- run_MCMC(x = sample_data$x, y = sample_data$y, coeff_inits = c(0,30,45,0.2), sdy_init = 4, nIter = nIter)\rTo assess the model output, we produce trace plots and density plots of the posterior estimates of the parameters. For the trace plot, only every \\(10^{th}\\) iteration is shown to improve readability. The black lines in the density plot denote the parameters of the original sea surface temperature gradient.\rThe parameters have converged reasonably well.\nBelow, we discard the first \\(10,000\\) iterations as burn-in and plot \\(8\\) gradients, using different samples from the posterior (blue lines, lefthand plot). As expected, they fit nicely to the \\(10\\) sampled data points (grey dots). They are also quite similar to the original gradient (black, dashed line). To the right, the estimated temperature gradient using the median of the parameters from the posterior (blue line), and \\(95\\%\\) credible intervals (blue shading), are shown. Between \\(10^\\circ\\) and \\(50^\\circ\\), where we have sufficient samples, the estimated gradient very closely resembles the original gradient. The constraints imposed by the priors ensure that the estimated sea surface temperature gradients stays in a realistic range (\\(\u0026gt;-4^\\circ C\\)), even at latitudes \\(\u0026gt; 63^\\circ\\) where we have no data.\nIn conclusion, the model seems to be doing a good job in estimating a sensible temperature gradient from sparse samples. In the second part of this series, we will implement adaption of \\(\\sigma_{proposal}\\), which means we won‚Äôt have to guess good values for \\(\\sigma_{proposal}\\). This should speed up convergence, meaning we will need less iterations of the MCMC algorithm to obtain reliable posterior estimates.\nYou can find the full R code to reproduce all analyses and figures on Github.\n","date":1644624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644700810,"objectID":"f74f260f6a35cf523ffa55ef4ae150b4","permalink":"/post/a-metropolis-algorithm-in-r-part-1-implementation/","publishdate":"2022-02-12T00:00:00Z","relpermalink":"/post/a-metropolis-algorithm-in-r-part-1-implementation/","section":"post","summary":"The Metropolis algorithm is a common MCMC method. Here, it is used for estimating a generalised logistic function to reconstruct a latitudinal climate gradient from a small sample of temperature values.","tags":[],"title":"A Metropolis algorithm in R - Part 1: Implementation","type":"post"},{"authors":[],"categories":[],"content":"\r\r\r.column-left{\rfloat: left;\rwidth: 52%;\rtext-align: left;\n}\n\r.column-center{\rfloat: center;\rwidth: 100%;\rtext-align: left;\r}\r.column-right{\rfloat: right;\rwidth: 48%;\rtext-align: left;\rmargin-top: 6px;\rline-height: 1.83;\rfont-size: 12px;\r}\r\rbody, td {\rfont-size: 14px;\r}\rcode.r{\rfont-size: 12px;\r}\rpre {\rfont-size: 13.2px\r}\r\r\rThis implementation of change point regression was developed by Julian Stander (University of Plymouth) in Eichenseer et al.¬†(2019).\n\rAssume we want to investigate the relationship between two variables, \\(x\\) and \\(y\\), that we have collected over a certain period of time. We have reason to believe that the relationship changed at some point, but we don‚Äôt know when.\nLet‚Äôs generate \\(x\\) and \\(y\\) and plot them. \\(y\\) is linearly dependent on \\(x\\) across the whole time series, but we induce an increase in the intercept, slope and residual variance at the \\(35^{th}\\) observation:\nset.seed(10) # change the seed for a different sequence of random numbers\rn \u0026lt;- 60 # number of total data points\rn_shift \u0026lt;- 35 # the data point at which we introduce a change\rx \u0026lt;- rnorm(n,0,1) # generate x\ry \u0026lt;- rnorm(n,0,0.5) + 0.5 * x # generate y without a change\ry[n_shift:n] \u0026lt;- rnorm(length(n_shift:n),0,1) + 1 * x[n_shift:n] + 0.75 # introduce change\rThe regression model\rNow we build a model that can recover the change point and the linear relationship between \\(x\\) and \\(y\\) before and after the change point.\nThe first part of this model looks like an ordinary least squares regression of \\(y\\) against \\(x\\):\n\\(\\begin{aligned} \\begin{equation} \\begin{array}{l} y_i \\sim N(\\mu_i, \\sigma_1^2), ~~\\\\ \\mu_i = \\alpha_1~+~\\beta_1~x_i, ~~~~~ i = 1,...,n_{change}-1 \\end{array} \\end{equation} \\end{aligned}\\)\nHere we have a single intercept (\\(\\alpha_1\\)), slope (\\(\\beta_1\\)), and residual variance (\\(\\sigma^2_1\\)). \\(n_{change}\\) - 1 denotes the number of obervations before the change point.\nFrom the change point \\(n_{change}\\) onwards, we add an additional intercept, \\(\\alpha_2\\), to the intercept from the first part (\\(\\alpha_1\\)). We do the same for the slope and the residual variance:\n\\(\\begin{aligned} \\begin{equation} \\begin{array}{l} y_i \\sim N(\\mu_i, \\sigma_1^2+\\sigma_2^2), ~~\\\\ \\mu_i = \\alpha_1~+~\\alpha_2~+~(\\beta_1~+~\\beta_2)~x_i, ~~~~~ i = n_{change},...,n \\end{array} \\end{equation} \\end{aligned}\\)\n\\(n\\) denotes the total number of observations, 60 in this case. But how do we actually find the change point \\(n_{change}\\)?\n\rImplementation in JAGS\rHere, we turn to the JAGS programming environment. Understanding a model written for JAGS is not easy at first. If you are keen on learning Bayesian modeling from scratch I can highly recommend Richard McElreath‚Äôs book Statistical Rethinking. We will access JAGS with the R2jags package, so we can keep using R even if we are writing a model for JAGS.\n\rBayesian methods for detecting change points are also available in Stan, as discussed here. An application using English league football data can be found here.\n\rBelow, we look at the model. The R code that will be passed to JAGS later is on the left. On the right is an explanation for each line of the model.\nWe save the model as a function named\nmodel_CPR\r\nLoop over all the data points \\(1,...,n\\)\n\\(y_i \\sim N(\\mu_i, \\tau_i)\\)\nnote that JAGS uses the precision \\(\\tau\\) instead\nof \\(\\sigma^2\\). \\(\\tau = 1/\\sigma^2\\)\nstep takes the value \\(1\\) if its argument is \\(\\ge 0\\),\nand \\(0\\) otherwise, resulting in\n\\(\\mu_i = \\alpha_1~+~\\beta_1~x_i\\) before \\(n_{change}\\) and\n\\(\\mu_i = \\alpha_1~+~\\alpha_2~+~(\\beta_1~+~\\beta_2)~x_i\\)\nfrom \\(n_{change}\\) onwards.\nback-transform \\(\\log(\\tau)\\) to \\(\\tau\\).\nagain, the step function is used to define \\(\\log(\\tau)\\) before and after \\(n_{change}\\). Log-transformation is used to ensure that the \\(\\tau\\) resulting from \\(\\tau_1\\) and \\(\\tau_2\\) is positive.\nWe have to define priors for all parameters that are not specified by data.\n\\(\\alpha_1 \\sim N(\\mu = 0, \\tau = 10^{-4})\\) That is a normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 100\\),\nbecause \\(\\sigma = 1/\\sqrt{\\tau}\\)\n\\(\\alpha_2 \\sim N(0, 10^{-4})\\)\n\\(\\beta_1 \\sim N(0, 10^{-4})\\)\n\\(\\beta_2 \\sim N(0, 10^{-4})\\)\n\\(\\log(\\tau_1) \\sim N(0, 10^{-4})\\)\n\\(\\log(\\tau_2) \\sim N(0, 10^{-4})\\)\nDiscrete prior on the change point. \\(K\\) indicates one of the possible change points,\rbased on the probability vector \\(p\\), which we need to specify beforehand.\n\rmodel_CPR \u0026lt;- function(){\r### Likelihood or data model part\rfor(i in 1:n){\ry[i] ~ dnorm(mu[i], tau[i]) mu[i] \u0026lt;- alpha_1 + alpha_2 * step(i - n_change) +\r(beta_1 + beta_2 * step(i - n_change))*x[i]\rtau[i] \u0026lt;- exp(log_tau[i])\rlog_tau[i] \u0026lt;- log_tau_1 + log_tau_2 * step(i - n_change)\r} ### Priors\ralpha_1 ~ dnorm(0, 1.0E-4)\ralpha_2 ~ dnorm(0, 1.0E-4)\rbeta_1 ~ dnorm(0, 1.0E-4)\rbeta_2 ~ dnorm(0, 1.0E-4)\rlog_tau_1 ~ dnorm(0, 1.0E-4)\rlog_tau_2 ~ dnorm(0, 1.0E-4)\rK ~ dcat(p)\rn_change \u0026lt;- possible_change_points[K]\r}\rNote that we put priors on \\(\\log(\\tau_1)\\) and \\(\\log(\\tau_2)\\), rather than on \\(\\tau_1\\) and \\(\\tau_2\\) directly, to ensure that the precision \\(\\tau\\) in the second part of the regression always remains positive. \\(e^{\\log(\\tau_1) + \\log(\\tau_2)}\\) is always \\(\u0026gt; 0\\), even if the term \\(\\log(\\tau_1)\\) + \\(\\log(\\tau_2)\\) becomes negative.\nPrepare the data which we pass to JAGS along with the model:\n# minimum number of the data points before and after the change\rmin_segment_length \u0026lt;- 5 # assign indices to the potential change points we allow\rpossible_change_points \u0026lt;- (1:n)[(min_segment_length+1):(n+1-min_segment_length)] # number of possible change points\rM \u0026lt;- length(possible_change_points) # probabilities for the discrete uniform prior on the possible change points, # i.e. all possible change points have the same prior probability\rp \u0026lt;- rep(1 / M, length = M) # save the data to a list for jags\rdata_CPR \u0026lt;- list(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;, \u0026quot;n\u0026quot;, \u0026quot;possible_change_points\u0026quot;, \u0026quot;p\u0026quot;) \rLoad the R2jags package to access JAGS in R:\n library(R2jags) \rNow we execute the change point regression. We instruct JAGS to run three seperate chains so we can verify that the results are consistent. We allow 2000 iterations of the Markov chain Monte Carlo algorithm for each chain, the first 1000 of which will automatically be discarded as burn-in.\n CPR \u0026lt;- jags(data = data_CPR, parameters.to.save = c(\u0026quot;alpha_1\u0026quot;, \u0026quot;alpha_2\u0026quot;, \u0026quot;beta_1\u0026quot;,\u0026quot;beta_2\u0026quot;,\r\u0026quot;log_tau_1\u0026quot;,\u0026quot;log_tau_2\u0026quot;,\r\u0026quot;n_change\u0026quot;), n.iter = 2000, n.chains = 3,\rmodel.file = model_CPR)\r\rThe results\rTo visualise the results and inspect the posterior, we are using the ggmcmc package, which relies on the ggplot2 package. For brevity, we just look at the \\(n_{change}\\) parameter here.\nlibrary(ggmcmc)\rCPR.ggs \u0026lt;- ggs(as.mcmc(CPR)) # convert to ggs object\rggs_traceplot(CPR.ggs, family = \u0026quot;n_change\u0026quot;) \rLooks like the chains converge and mix nicely. We can already see that our model locates the change point somewhere between \\(30\\) and \\(40\\), although the chains occasionally explore regions further away.\nLet‚Äôs look at the posterior probabilities for the possible change points:\nggplot(data = CPR.ggs %\u0026gt;% filter(Parameter == \u0026quot;n_change\u0026quot;),\raes(x=value, y = 3*(..count..)/sum(..count..), fill = as.factor(Chain))) + geom_vline(xintercept = 35,lty = 2) + geom_bar(position = \u0026quot;identity\u0026quot;, alpha = 0.5) +\rylab(\u0026quot;posterior probability\u0026quot;) + xlab(\u0026quot;n_change\u0026quot;) + labs(fill=\u0026#39;Chain\u0026#39;)\rThe \\(37^{th}\\) point has the highest probability of being the change point. That is not far off from where we introduced the change, at the \\(35^{th}\\) point (dashed line). The random generation of \\(x\\) and \\(y\\) has led to \\(37\\) being favoured. We also note that there are only minor differences between the three chains, and those differences would likely further dwindle if we were to let the chains run for longer.\nUsing the posterior distribution, we can answer questions like: ‚ÄúIn which interval does the change point fall with 90 % probability?‚Äù\nquantile(CPR$BUGSoutput$sims.list$n_change, probs = c(0.05, 0.95))\r## 5% 95% ## 33 39\rWe can also inquire about the probability that the change point falls in the interval \\(34\\) to \\(38\\):\nround(length(which(CPR$BUGSoutput$sims.list$n_change %in% 34:38))/\r(CPR$BUGSoutput$n.sims),2)\r## [1] 0.87\rFinally, let‚Äôs have a look at the regression parameters and plot the resulting regressions before and after the most likely change point.\nThe intercept, slope, and residual variance all increase after the change point.\nThis can be immediately seen when plotting the change point regression:\rThe shaded areas denote \\(95\\) % credible intervals around the regression lines.\nYou can find the full R code for this analysis at https://github.com/KEichenseer/Methods/blob/main/Change_point_regression.R\nGet in touch if you have any comments or questions!\n\r","date":1627430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627462729,"objectID":"88ac832bdb2c666a211891011443bad9","permalink":"/post/change-point-regression/","publishdate":"2021-07-28T00:00:00Z","relpermalink":"/post/change-point-regression/","section":"post","summary":"Assume we want to investigate the relationship between two variables, $x$ and $y$, that we have collected over a certain period of time. We have reason to believe that the relationship changed at some point, but we don't know when.","tags":[],"title":"Change Point Regression","type":"post"},{"authors":["admin"],"categories":null,"content":"\rCreate your slides in Markdown - click the Slides button to check out the example.\r\r\rSupplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["admin","Robert Ford"],"categories":null,"content":"\rClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.\r\r\r\rCreate your slides in Markdown - click the Slides button to check out the example.\r\r\rSupplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["admin","Robert Ford"],"categories":null,"content":"\rClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.\r\r\r\rCreate your slides in Markdown - click the Slides button to check out the example.\r\r\rSupplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]