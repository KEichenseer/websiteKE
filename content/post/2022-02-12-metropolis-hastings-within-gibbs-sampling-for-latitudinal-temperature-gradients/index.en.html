---
title: "A Metropolis algorithm in R - Part 1: Implementation"
author: "Kilian Eichenseer"
date: '2022-02-12'
slug: a-metropolis-algorithm-in-R-part-1-implementation
categories: []
tags: []
subtitle: ''
authors: []
lastmod: '2022-02-12T21:20:10Z'
#featured: false
image:
  caption: ''
  focal_point: ''
  preview_only: true
projects: []
header-includes: \usepackage{graphics}
summary: The Metropolis algorithm is a common MCMC method. Here, it is used for estimating a generalised logistic function to reconstruct a latitudinal climate gradient from a small sample of temperature values.
---




<style>
.math {
  font-size: small;
}
</style>
<style>
<p>.column-left{
float: left;
width: 52%;
text-align: left;</p>
<p>}</p>
<style>
.column-center{
  float: center;
  width: 100%;
  text-align: left;

}

.column-right{
  float: right;
  width: 48%;
  text-align: left;
  margin-top: 6px;
  line-height: 1.83;
  font-size: 12px;

}
</style>
<style type="text/css">
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 13.2px
}
</style>
<blockquote>
<p><em>The model presented herein uses modified code from <a href="https://khayatrayen.github.io/MCMC.html" class="uri">https://khayatrayen.github.io/MCMC.html</a>. I am currently developing a Metropolis-within-Gibbs algorithm for stratigraphic correlation of <span class="math inline">\(\delta\)</span><sup>13</sup>C records with Andrew R. Millard and Martin R. Smith at the <a href="https://smithlabdurham.github.io/#!team">Smith Lab at Durham University</a>.</em></p>
</blockquote>
<p>Markov chain Monte Carlo (MCMC) methods are widely used to obtain posterior probabilities for the unknown parameters of Bayesian models. The <a href="https://arxiv.org/pdf/1504.01896.pdf">Metropolis algorithm</a> builds a Markov chain for each parameter, which resembles the posterior distribution. This works by selecting arbitrary starting values for the parameters and calculating the resulting joint posterior probability. Then, new values for the parameters are randomly proposed, and the joint posterior probability is calculated with the new parameter values. If the posterior obtained with the new values is higher than that of the current values, the new values will be recorded and added to the Markov chains. Otherwise, the new value will be accepted with a probability equal to the ratio of the two posterior probabilities. If the proposed values result in a much lower posterior probability than the current values, the proposal will most likely be rejected. This process is repeated many times, and the resulting Markov chains converge on the posterior distributions of the parameters. The Metropolis algorithm requires symmetric proposal distributions and is a special case of the Metropolis-Hastings algorithm.</p>
<p>To illustrate the implementation of the Metropolis algorithm, we turn to climatology: Latitudinal temperature gradients from Earth history are difficult to reconstruct due to the <a href="https://www.lewisajones.com/post/uneven-spatial-sampling-and-reconstructing-global-palaeotemperatures/">sparse and geographically variable sampling of proxy data in most geological intervals</a>. To reconstruct plausible temperature gradients from a fragmentary proxy record, classical solutions like LOESS or standard generalised additive models are not optimal, as earth scientists have additional information on past temperature gradients that those models do not incorporate. Instead, I propose the use of a generalised logistic function (a modified <a href="https://www.jstor.org/stable/23686557?seq=1#metadata_info_tab_contents">Richard’s curve</a>) that can readily incorporate information in addition to the proxy data. For example, we can instruct the model to force temperature to continuously decrease from the tropics toward the poles.</p>
<p>To keep with the familiar notation in regression models, we set denote latitude as <span class="math inline">\(x\)</span> and temperature as <span class="math inline">\(y\)</span>. Temperature is modelled as a function of latitude as:</p>
<p><span class="math inline">\(\begin{aligned} \begin{equation} \begin{array}{l} y_i \sim N(\mu_i, \sigma), ~~\\ \mu_i = A~+max(K-A,0)/(e^{Q(x_i-M)}), ~~~~~ i = 1,...,n. \end{array} \end{equation} \end{aligned}\)</span></p>
<p><span class="math inline">\(A\)</span> is the lower asymptote, <span class="math inline">\(K\)</span> is the upper asymptote, <span class="math inline">\(M\)</span> is the inflection point, i.e. the steepest point of the curve, and <span class="math inline">\(Q\)</span> controls the steepness of the curve. The difference <span class="math inline">\(K-A\)</span> is constrained to be <span class="math inline">\(\ge 0\)</span> to preclude inverse temperature gradients.</p>
<p>In R code, we turn this into a function named <span class="math inline">\(gradient\)</span>:</p>
<pre class="r"><code>gradient &lt;- function(x, coeff, sdy) { # sigma is labelled &quot;sdy&quot;
  A = coeff[1]
  K = coeff[2]
  M = coeff[3]
  Q = coeff[4]
  return(A + max(c(K-A,0))/((1+(exp(Q*(x-M))))) + rnorm(length(x),0,sdy))
}</code></pre>
<p>As an example, let’s look at the modern, average latitudinal sea surface temperature gradient. We approximate it by setting <span class="math inline">\(A = -2.0\)</span>, <span class="math inline">\(K = 28\)</span>, <span class="math inline">\(M = 41\)</span>, and <span class="math inline">\(Q = 0.10\)</span>. The residual standard deviation <span class="math inline">\(\sigma\)</span> is set to <span class="math inline">\(0\)</span>, resulting in a smooth curve without noise (lefthand plot). Note that we are using absolute latitudes, assuming a common latitudinal temperature gradient in both hemispheres.
We also sample <span class="math inline">\(10\)</span> points from this gradient, introducing some noise by setting <span class="math inline">\(\sigma = 2\)</span> (righthand plot). In the following, we will use these <span class="math inline">\(10\)</span> points to estimate a latitudinal gradient, using the gradient model specified above.</p>
<pre class="r"><code>set.seed(10)
sample_lat &lt;- runif(10,0,90)
sample_data &lt;- data.frame(
  x = sample_lat, 
  y = gradient(x = sample_lat, coeff = c(-2.0, 28, 41, 0.1), sd = 2))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-3-1.png" width="864" /></p>
<p>Before writing the main Markov chain Monte Carlo (MCMC) function, we pre-define a couple of supplementary functions that we use in every iteration of the Metropolis algorithm.</p>
<p>We start with the log-likelihood function, which translates to the joint probability of the data, given a specific set of model parameters:</p>
<pre class="r"><code>loglik &lt;- function(x, y,  coeff, sdy) {
  A = coeff[1]
  K = coeff[2]
  M = coeff[3]
  Q = coeff[4]
  pred = A + max(c(K-A,0))/((1+(exp(Q*(x-M)))))
  return(sum(dnorm(y, mean = pred, sd = sdy, log = TRUE)))
}</code></pre>
<p>Next, we need a function to generate the log-priors, i.e. the joint prior probability of the set of model parameters. We specify the parameters of the prior distribution within the function for convenience. Uniform priors ranging from <span class="math inline">\(-4\)</span> to <span class="math inline">\(40\)</span> are put on <span class="math inline">\(A\)</span> and <span class="math inline">\(K\)</span>, signifying that the temperature gradient cannot exceed this range. A normal prior with a mean of <span class="math inline">\(45\)</span> and a standard deviation of <span class="math inline">\(10\)</span> is placed on <span class="math inline">\(M\)</span>, implying that we expect the steepest temperature gradient in the mid-latitudes. We constrain <span class="math inline">\(Q\)</span> to be <span class="math inline">\(&gt;0\)</span> by placing a log-normal prior on it:</p>
<pre class="r"><code>logprior &lt;- function(coeff) {
  return(sum(c(
    dunif(coeff[1], -4, 40, log = TRUE),
    dunif(coeff[2], -4, 40, log = TRUE),
    dnorm(coeff[3], 45, 10, log = TRUE),
    dlnorm(coeff[4], -2, 1, log = TRUE))))
}</code></pre>
<p>The posterior is proportional to the likelihood <span class="math inline">\(\times\)</span> prior. On the log scale, we can simply add them:</p>
<pre class="r"><code>logposterior &lt;- function(x, y, coeff, sdy){
  return (loglik(x, y, coeff, sdy) + logprior(coeff))
}</code></pre>
<p>Finally, we define a function that proposes new values for the Metropolis-Hastings step. The magnitude of the proposal standard deviations (<span class="math inline">\(\sigma_{proposal}\)</span>) is quite important, as low values will lead to the chain exploring the parameter space very slowly, and high values result in a low acceptance rate and an insufficient exploration of the parameter space. As appropriate <span class="math inline">\(\sigma_{proposal}\)</span> are difficult to know <em>a priori</em>, adaptive steps are often used to find better values. For simplicity, we will use fixed <span class="math inline">\(\sigma_{proposal}\)</span>. Different <span class="math inline">\(\sigma_{proposal}\)</span> can and usually should be used for different parameters.</p>
<pre class="r"><code>MH_propose &lt;- function(coeff, proposal_sd){
  return(rnorm(4,mean = coeff, sd= c(.5,.5,.5,0.01)))
}</code></pre>
<p>With all the prerequisites in place, we can build the MCMC function. The model will update <span class="math inline">\(\sigma\)</span> with a Gibbs step, and update the other coefficients with a Metropolis-Hastings step:</p>
<pre class="r"><code>run_MCMC &lt;- function(x, y, coeff_inits, sdy_init, nIter){
  ### Initialisation
  coefficients = array(dim = c(nIter,4)) # set up array to store coefficients
  coefficients[1,] = coeff_inits # initialise coefficients
  sdy = rep(NA_real_,nIter) # set up vector to store sdy
  sdy[1] = sdy_init # intialise sdy
  A_sdy = 3 # parameter for the prior on the inverse gamma distribution of sdy
  B_sdy = 0.1 # parameter for the prior on the inverse gamma distribution of sdy
  n &lt;- length(y)
  shape_sdy &lt;- A_sdy+n/2 # shape parameter for the inverse gamma
  
  ### The MCMC loop
  for (i in 2:nIter){ 
    
    ## 1. Gibbs step to estimate sdy
    sdy[i] = sqrt(1/rgamma(
      1,shape_sdy,B_sdy+0.5*sum((y-gradient(x,coefficients[i-1,],0))^2)))
    
    ## 2. Metropolis-Hastings step to estimate the regression coefficients
    proposal = MH_propose(coefficients[i-1,]) # new proposed values
    if(any(proposal[4] &lt;= 0)) HR = 0 else # Q needs to be &gt;0
    # Hastings ratio of the proposal
    HR = exp(logposterior(x = x, y = y, coeff = proposal, sdy = sdy[i]) -
             logposterior(x = x, y = y, coeff = coefficients[i-1,], sdy = sdy[i]))
    # accept proposal with probability = min(HR,1)
    if (runif(1) &lt; HR){ 
      coefficients[i,] = proposal
    # if proposal is rejected, keep the values from the previous iteration
    }else{
      coefficients[i,] = coefficients[i-1,]
    }
  } # end of the MCMC loop
  
  ###  Function output
  output = data.frame(A = coefficients[,1],
                      K = coefficients[,2],
                      M = coefficients[,3],
                      Q = coefficients[,4],
                      sdy = sdy)
  return(output)
}</code></pre>
<p>To run the model, we need to provide starting values for the unknown parameters. We let it run for <span class="math inline">\(100,000\)</span> iterations:</p>
<pre class="r"><code>nIter &lt;- 100000
m &lt;- run_MCMC(x = sample_data$x, y = sample_data$y, 
              coeff_inits = c(0,30,45,0.2), sdy_init = 4, nIter = nIter)</code></pre>
<p>To assess the model output, we produce trace plots and density plots of the posterior estimates of the parameters. For the trace plot, only every <span class="math inline">\(10^{th}\)</span> iteration is shown to improve readability. The black lines in the density plot denote the parameters of the original sea surface temperature gradient.
<img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-10-1.png" width="691.2" />
The parameters have converged reasonably well.</p>
<p>Below, we discard the first <span class="math inline">\(10,000\)</span> iterations as burn-in and plot <span class="math inline">\(8\)</span> gradients, using different samples from the posterior (blue lines, lefthand plot). As expected, they fit nicely to the <span class="math inline">\(10\)</span> sampled data points (grey dots). They are also quite similar to the original gradient (black, dashed line). To the right, the estimated temperature gradient using the median of the parameters from the posterior (blue line), and <span class="math inline">\(95\%\)</span> credible intervals (blue shading), are shown. Between <span class="math inline">\(10^\circ\)</span> and <span class="math inline">\(50^\circ\)</span>, where we have sufficient samples, the estimated gradient very closely resembles the original gradient. The constraints imposed by the priors ensure that the estimated sea surface temperature gradients stays in a realistic range (<span class="math inline">\(&gt;-4^\circ C\)</span>), even at latitudes <span class="math inline">\(&gt; 63^\circ\)</span> where we have no data.</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-11-1.png" width="864" />
In conclusion, the model seems to be doing a good job in estimating a sensible temperature gradient from sparse samples. In the <a href="/post/a-metropolis-algorithm-in-r-part-2-adaptive-proposals">second part</a> of this series, we will implement adaption of <span class="math inline">\(\sigma_{proposal}\)</span>, which means we won’t have to guess good values for <span class="math inline">\(\sigma_{proposal}\)</span>. This should speed up convergence, meaning we will need less iterations of the MCMC algorithm to obtain reliable posterior estimates.</p>
<p>You can find the full R code to reproduce all analyses and figures on <a href="https://github.com/KEichenseer/Methods/blob/main/A_Metropolis_algorithm_in_R_generalised_logistic_function.R">Github</a>.</p>
