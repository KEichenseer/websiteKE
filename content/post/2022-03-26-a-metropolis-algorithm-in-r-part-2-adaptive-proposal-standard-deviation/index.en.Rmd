---
title: "A Metropolis algorithm in R - Part 2: Adaptive proposals"
author: ''
date: "2022-03-26"
output: pdf_document
categories: []
tags: []
subtitle: ''
authors: []
lastmod: "2022-03-26T20:13:04Z"
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
header-includes: \usepackage{graphics}
summary: The Metropolis algorithm is a simple, but powerful MCMC method. Here, I use
  it for estimating a generalised logistic function to reconstruct a latitudinal climate
  gradient from a small sample of temperature estimates.
slug: "a-metropolis-algorithm-in-r-part-2-adaptive-proposals"
---

<style>
.math {
  font-size: small;
}
</style>


<style type="text/css">
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 13.2px
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
> *I am currently developing a Metropolis-within-Gibbs algorithm for stratigraphic correlation of $\delta$<sup>13</sup>C records with Andrew R. Millard and Martin R. Smith at the [Smith Lab at Durham University](https://smithlabdurham.github.io/#!team).*

In the [previous post](/post/a-metropolis-algorithm-in-r-part-1-implementation), we built a Metropolis algorithm to estimate latitudinal temperature gradients, approximated by a generalised logistic function. Recall that the Metropolis algorithm works by proposing new parameter values and evaluating the joint posterior probability of the model with these values, against the posterior with the current values.

How do we chose a new value for a parameter? A common approach is to sample a normal distribution, centred at the current value (i.e. the mean of the distribution is the current value). Choosing the standard deviation of the proposal distribution ($\sigma_{proposal}$) is more tricky. If $\sigma_{proposal}$ is too high, we end up proposing a lot of values at the far tail ends of the target posterior distribution, which will usually be rejected (see below, green proposals). This leads to inefficient sampling and patchy coverage of the posterior distribution. Conversely, a very small $\sigma_{proposal}$ leads to most new values being accepted, but the resulting Markov chain will move very slowly through the parameter space, leading to a low effective sample size (red proposals below). Instead, some intermediate $\sigma_{proposal}$ is desirable, at which the Markov chain moves quickly through the parameter space, without too many rejections (e.g., yellow proposals below). 

```{r, fig.width = 7, fig.height = 4, warning = FALSE, echo = FALSE}

set.seed(2)
par(mfrow=c(2,1),mar = c(0.5,4,0,0), las = 1)
xd <- seq(-12.2,12.2,0.01)
yd <- dnorm(xd,-1.5,1)+dnorm(xd,1.5,1)
plot(xd,yd, xlab = "", ylab = "density",type = "n", yaxs = "i", xaxs = "i", ylim = c(0,0.43), xaxt = "n", yaxt = "n")
axis(2,seq(0,0.5,0.1), c("0",as.character(seq(0.1,0.4,0.1)),NA))
text(-11.5,0.39,"target distribution", adj = c(0,.5), cex = 1.1)

error_polygon <- function(x,en,ep,color) {
  polygon( c(x[1], x, x[length(x)], x[length(x)], rev(x), x[1]),
           c((ep)[1],ep, (ep)[length(ep)], (en)[length(en)], rev(en), (en)[1]),
           border = NA, col = color)
}

col3 <- rgb(0.8,0.8,0,.75)
col2 <- rgb(0.8,0.5,0,.75)
col1 <- rgb(1,0,0,.75)


error_polygon(xd,rep(0,length(xd)),yd,rgb(0,0.35,0.7,0.33))
p1 <- -0.75
sd3 <- p1+rnorm(10,0,0.2)
points(sd3,dnorm(sd3,-1.5,1)+dnorm(sd3,1.5,1), pch = 4, cex = 1.2, lwd = 2, col = col1, xpd = T)

sd2 <- p1+rnorm(10,0,2.5)
points(sd2,dnorm(sd2,-1.5,1)+dnorm(sd2,1.5,1), pch = 4, cex = 1.2, lwd = 2, col = col2, xpd = T)

sd1 <- p1+rnorm(10,0,8)
points(sd1,dnorm(sd1,-1.5,1)+dnorm(sd1,1.5,1), pch = 4, cex = 1.2, lwd = 2, col = col3, xpd = T)

abline(v= -0.75, lty = 3, lwd = 2)
points(-0.75,dnorm(-0.75,-1.5,1)+dnorm(-0.75,1.5,1), pch = 4, cex = 1.75, lwd = 3, col = rgb(0,0,0,0.67), xpd = T)
sigmas <- c(8,2.5,0.2)
legend("topright", as.expression(c("current value",sapply(1:3, function(x) bquote(italic(sigma["proposal"]~"="~.(sigmas[x])))))), 
       col = c(rgb(0,0,0,0.67),col1,col2,col3), border = NA, bty = "n", pch = 4, 
       pt.cex = c(1.7,1.2,1.2,1.2), pt.lwd = c(3,2,2,2), cex = 0.95)


par(mar = c(2,4,0.5,0))

plot(xd,yd, xlab = "", ylab = "density (not to scale)",type = "n", yaxs = "i", xaxs = "i", ylim = c(0,0.43), yaxt = "n")
text(-11.5,0.39,"proposal distributions", adj = c(0,.5), cex = 1.1)
axis(2,seq(0,0.5,0.1), c("0",as.character(seq(0.1,0.4,0.1)),NA))

col1 <- rgb(0.8,0.8,0,0.35)
col2 <- rgb(0.8,0.5,0,0.4)
col3 <- rgb(1,0,0,0.4)

error_polygon(xd,rep(0,length(xd)),dnorm(xd,p1,8)*4,col1)
error_polygon(xd,rep(0,length(xd)),dnorm(xd,p1,2.5)*1.8,col2)
error_polygon(xd,rep(0,length(xd)),dnorm(xd,p1,0.2)*0.2,col3)


abline(v= -0.75, lty = 3, lwd = 2)
legend("topright", as.expression(sapply(1:3, function(x) bquote(italic(sigma["proposal"]~"="~.(sigmas[x]))))), 
       fill = c(col3,col2,col1), border = NA, bty = "n", cex = 0.95)


```

It turns out that the Metropolis algorithm is usually most efficient when the acceptance rate of proposals is between $ 0.2$ and $0.5$ (see [Roberts and Rosenthal 2001](https://www.jstor.org/stable/pdf/3182776.pdf?casa_token=L8a1gJYi1SgAAAAA:ZVa-bCWzwBW3vAat13KRVDkDRu63BWmdxddvp2xLGAjV0bt1j72SP_tEXsxJrU1GqRDyu_23QMDMnCrMJM9Ydrc3bUAylT9eeJeqs5cmPrk9EFIiq9i-)). In practice, this could be achieved e.g. by monitoring the acceptance rate or the standard deviation of the target distribution ($\sigma_{target}$), and adjusting $\sigma_{proposal}$ accordingly. For a four-dimensional Gaussian target distribution, the optimal $\sigma_{proposal}$ should be around $~2.4/sqrt(4) \times \sigma_{target}$ ([Gelman et al. 1996](http://people.ee.duke.edu/~lcarin/baystat5.pdf)). Our target distributions are not exactly Gaussian, but we will try adapting $\sigma_{proposal}$ and check whether we achieve reasonable acceptance rates.

In order to allow for $\sigma_{proposal}$ to quickly converge on the optimum, the weighted variance of the samples from the Markov chains from previous iterations is calculated. The weights decrease backwards in time, so that the recent values have more influence on the new value for $\sigma_{proposal}$. The weighted variance is calculated with the following function, where `x` denotes a vector of samples from the Markov chain, weights a vector of `weights` (see below), and `sum_weights` records the sum of the weights vector:

```{r, warning = FALSE, echo = TRUE}
weighted_var <- function(x, weights, sum_weights) {
  sum(weights*((x-sum(weights*x)/sum_weights)^2))/(sum_weights)
}
```

We can re-use most of the auxiliary functions of the [standard Metropolis algorithm](/post/a-metropolis-algorithm-in-r-part-1-implementation). Notably, the value of `propose_sd` will change during the adaptation. 
```{r, warning = FALSE, echo = FALSE}
gradient <- function(x, coeff, sdy) { # sigma is labelled "sdy"
  A = coeff[1]
  K = coeff[2]
  M = coeff[3]
  Q = coeff[4]
  return(A + max(c(K-A,0))/((1+(exp(Q*(x-M))))) + rnorm(length(x),0,sdy))
}

loglik <- function(x, y,  coeff, sdy) {
  A = coeff[1]
  K = coeff[2]
  M = coeff[3]
  Q = coeff[4]
  pred = A + max(c(K-A,0))/((1+(exp(Q*(x-M)))))
  return(sum(dnorm(y, mean = pred, sd = sdy, log = TRUE)))
}

logprior <- function(coeff) {
  return(sum(c(
    dunif(coeff[1], -4, 40, log = TRUE),
    dunif(coeff[2], -4, 40, log = TRUE),
    dnorm(coeff[3], 45, 10, log = TRUE),
    dlnorm(coeff[4], -2, 1, log = TRUE))))
}

logposterior <- function(x, y, coeff, sdy){
  return (loglik(x, y, coeff, sdy) + logprior(coeff))
}

```

```{r, warning = FALSE, echo = TRUE}
MH_propose <- function(coeff, proposal_sd){
  rnorm(4,mean = coeff, sd= proposal_sd)
}
```

The specification of the weights and the adaption of $\sigma_{proposal}$ is implemented in the updated MCMC loop. `nAdapt` specifies the number of iterations in which adaptations takes place. These iterations need to be discarded as burn-in to not bias the estimate of the posterior. `adaptation_decay` is a constant that influences the exponential decay of the weights for the weighted variance function, with larger values leading to slower decay.

Below is the full MCMC loop:

```{r, warning = FALSE, echo = TRUE}
# Main MCMCM function
run_MCMC <- function(x, y, coeff_inits, sdy_init, nIter, proposal_sd_init = rep(5,4), 
                     nAdapt = 5000, adaptation_decay = 500){
  ### Initialisation
  coefficients = array(dim = c(nIter,4)) # set up array to store coefficients
  coefficients[1,] = coeff_inits # initialise coefficients
  sdy = rep(NA_real_,nIter) # set up vector to store sdy
  sdy[1] = sdy_init # intialise sdy
  A_sdy = 3 # parameter for the prior on the inverse gamma distribution of sdy
  B_sdy = 0.1 # parameter for the prior on the inverse gamma distribution of sdy
  n <- length(y)
  shape_sdy <- A_sdy+n/2 # shape parameter for the inverse gamma
  sd_it <- 1 # iteration index for the proposal standard deviation
  coeff_sd <- array(NA_real_,dim = c(nAdapt,4)) # array to store proposal SDs
  coeff_sd[1:3,] <- proposal_sd_init # proposal SDs before adaptation
  # pre-define exp. decaying weights for weighted variance
  allWeights <- exp((-(nAdapt-2)):0/adaptation_decay) 
  accept <- rep(NA,nIter) # vector to store the acceptance or rejection of proposals
  ### The MCMC loop
  for (i in 2:nIter){

   ## 1. Gibbs step to estimate sdy
    sdy[i] = sqrt(1/rgamma(
      1,shape_sdy,B_sdy+0.5*sum((y-gradient(x,coefficients[i-1,],0))^2)))

   ## 2. Metropolis-Hastings step to estimate the regression coefficients
    proposal = MH_propose(coefficients[i-1,],coeff_sd[sd_it,]) # new proposed values
    if(any(proposal[4] <= 0)) HR = 0 else {# Q and nu need to be >0
      # Hasting's ratio of the proposal
      HR = exp(logposterior(x = x, y = y, coeff = proposal, sdy = sdy[i]) -
                 logposterior(x = x, y = y, coeff = coefficients[i-1,], sdy = sdy[i]))}

    #if(gradient(65, proposal,0) >10) HR = 0
    # accept proposal with probability = min(HR,1)
    if (runif(1) < HR){
      accept[i] <- 1
      coefficients[i,] = proposal
      # if proposal is rejected, keep the values from the previous iteration
    }else{
      accept[i] <- 0
      coefficients[i,] = coefficients[i-1,]
    }
    # Adaptation of proposal SD
    if(i < nAdapt){ # stop adaptation after nAdapt iterations
    if(i>=3) {
    weights = allWeights[(nAdapt-i+2):nAdapt-1] # select weights for current iteration
    sum_weights = sum(weights) 
    weighted_var_coeff <- apply(coefficients[2:i,], 2, # calculate weighted variance
          function(f) weighted_var(
          f, weights = weights, sum_weights = sum_weights))


    for(v in 1:4) {if(weighted_var_coeff[v]==0)   { # 
              coeff_sd[i+1,v] <- sqrt(coeff_sd[i,v]^2/10)
      } else  coeff_sd[i+1,v] <- 2.4/sqrt(4) * sqrt(weighted_var_coeff[v])
    }                   
                           
    }
    sd_it <- i+1
    }
  } # end of the MCMC loop

  ###  Function output
  output = list(data.frame(A = coefficients[,1],
                           K = coefficients[,2],
                           M = coefficients[,3],
                           Q = coefficients[,4],
                           sdy = sdy),
                coeff_sd,
                accept)
  return(output)
}
```

```{r, warning = FALSE, echo = TRUE}
### Taking samples
set.seed(9)
sample_lat <- runif(10,0,90)
sample_data <- data.frame(
  x = sample_lat, 
  y = gradient(x = sample_lat, coeff = c(-2.0, 28, 41, 0.1), sd = 2))

### Analysis
nIter <- 100000
print(system.time({m7 <- run_MCMC(x = sample_data$x, y = sample_data$y,
                                 coeff_inits = c(0,30,45,0.2), sdy_init = 4, 
                                 nIter = nIter, nAdapt = 5000, adaptation_decay =1000,
                                 proposal_sd_init = c(1,1,1,0.1))}))

```

$1/4$ [(Gelman et al. 1997)](https://projecteuclid.org/journals/annals-of-applied-probability/volume-7/issue-1/Weak-convergence-and-optimal-scaling-of-random-walk-Metropolis-algorithms/10.1214/aoap/1034625254.full). 


[source 2](https://www.tandfonline.com/doi/abs/10.1198/jcgs.2009.06134)
