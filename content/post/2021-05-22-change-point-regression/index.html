---
title: "Change Point Regression"
author: ''
date: '2021-07-28'
slug: change-point-regression
categories: []
tags: []
subtitle: ''
authors: []
lastmod: '2021-07-28T10:58:49+02:00'
#featured: false
image:
  caption: ''
  focal_point: ''
  preview_only: true
projects: []
header-includes: \usepackage{graphics}
summary: Assume we want to investigate the relationship between two variables, $x$
  and $y$, that we have collected over a certain period of time. We have reason to
  believe that the relationship changed at some point, but we don't know when.
---




<style>
<p>.column-left{
float: left;
width: 52%;
text-align: left;</p>
<p>}</p>
<style>
.column-center{
  float: center;
  width: 100%;
  text-align: left;

}

.column-right{
  float: right;
  width: 48%;
  text-align: left;
  margin-top: 6px;
  line-height: 1.83;
  font-size: 12px;

}
</style>
<style type="text/css">
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 13.2px
}
</style>
<blockquote>
<p><em>This implementation of change point regression was developed by <a href="https://www.plymouth.ac.uk/staff/julian-stander">Julian Stander</a> (University of Plymouth) in <a href="https://www.nature.com/articles/s41561-019-0392-9">Eichenseer et al. (2019)</a>.</em></p>
</blockquote>
<p>Assume we want to investigate the relationship between two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, that we have collected over a certain period of time. We have reason to believe that the relationship changed at some point, but we don’t know when.</p>
<p>Let’s generate <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and plot them. <span class="math inline">\(y\)</span> is linearly dependent on <span class="math inline">\(x\)</span> across the whole time series, but we induce an increase in the intercept, slope and residual variance at the <span class="math inline">\(35^{th}\)</span> observation:</p>
<pre class="r"><code>set.seed(10) # change the seed for a different sequence of random numbers
n &lt;- 60 # number of total data points
n_shift &lt;- 35 # the data point at which we introduce a change
x &lt;- rnorm(n,0,1) # generate x
y &lt;- rnorm(n,0,0.5) + 0.5 * x # generate y without a change
y[n_shift:n] &lt;- rnorm(length(n_shift:n),0,1) + 1 * x[n_shift:n] + 0.75 # introduce change</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="960" /></p>
<div id="the-regression-model" class="section level2">
<h2>The regression model</h2>
<p>Now we build a model that can recover the change point and the linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> before and after the change point.</p>
<p>The first part of this model looks like an ordinary least squares regression of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>:</p>
<p><span class="math inline">\(\begin{aligned} \begin{equation} \begin{array}{l} y_i \sim N(\mu_i, \sigma_1^2), ~~\\ \mu_i = \alpha_1~+~\beta_1~x_i, ~~~~~ i = 1,...,n_{change}-1 \end{array} \end{equation} \end{aligned}\)</span></p>
<p>Here we have a single intercept (<span class="math inline">\(\alpha_1\)</span>), slope (<span class="math inline">\(\beta_1\)</span>), and residual variance (<span class="math inline">\(\sigma^2_1\)</span>). <span class="math inline">\(n_{change}\)</span> - 1 denotes the number of obervations before the change point.</p>
<p>From the change point <span class="math inline">\(n_{change}\)</span> onwards, we add an additional intercept, <span class="math inline">\(\alpha_2\)</span>, to the intercept from the first part (<span class="math inline">\(\alpha_1\)</span>). We do the same for the slope and the residual variance:</p>
<p><span class="math inline">\(\begin{aligned} \begin{equation} \begin{array}{l} y_i \sim N(\mu_i, \sigma_1^2+\sigma_2^2), ~~\\ \mu_i = \alpha_1~+~\alpha_2~+~(\beta_1~+~\beta_2)~x_i, ~~~~~ i = n_{change},...,n \end{array} \end{equation} \end{aligned}\)</span></p>
<p><span class="math inline">\(n\)</span> denotes the total number of observations, 60 in this case. But how do we actually find the change point <span class="math inline">\(n_{change}\)</span>?</p>
</div>
<div id="implementation-in-jags" class="section level2">
<h2>Implementation in JAGS</h2>
<p>Here, we turn to the <a href="https://mcmc-jags.sourceforge.io/">JAGS programming environment</a>. Understanding a model written for JAGS is not easy at first. If you are keen on learning Bayesian modeling from scratch I can highly recommend Richard McElreath’s book <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a>. We will access JAGS with the <a href="https://CRAN.R-project.org/package=R2jags">R2jags package</a>, so we can keep using R even if we are writing a model for JAGS.</p>
<blockquote>
<p><em>Bayesian methods for detecting change points are also available in <a href="https://mc-stan.org/">Stan</a>, as discussed <a href="https://mc-stan.org/docs/2_27/stan-users-guide/change-point-section.html">here</a>. An application using English league football data can be found <a href="https://www.significancemagazine.com/sports/693-has-english-league-football-become-less-exciting">here</a>.</em></p>
</blockquote>
<p>Below, we look at the model. The R code that will be passed to JAGS later is on the left. On the right is an explanation for each line of the model.</p>
<div class="column-right">
<p>We save the model as a function named<br />
<em>model_CPR</em>
<br/></p>
<p>Loop over all the data points <span class="math inline">\(1,...,n\)</span></p>
<p><span class="math inline">\(y_i \sim N(\mu_i, \tau_i)\)</span><br />
note that JAGS uses the precision <span class="math inline">\(\tau\)</span> instead<br />
of <span class="math inline">\(\sigma^2\)</span>.     <span class="math inline">\(\tau = 1/\sigma^2\)</span></p>
<p><em>step</em> takes the value <span class="math inline">\(1\)</span> if its argument is <span class="math inline">\(\ge 0\)</span>,<br />
and <span class="math inline">\(0\)</span> otherwise, resulting in<br />
<span class="math inline">\(\mu_i = \alpha_1~+~\beta_1~x_i\)</span>     before <span class="math inline">\(n_{change}\)</span> and<br />
<span class="math inline">\(\mu_i = \alpha_1~+~\alpha_2~+~(\beta_1~+~\beta_2)~x_i\)</span><br />
from <span class="math inline">\(n_{change}\)</span> onwards.</p>
<p>back-transform <span class="math inline">\(\log(\tau)\)</span> to <span class="math inline">\(\tau\)</span>.</p>
<p>again, the <em>step</em> function is used to define <span class="math inline">\(\log(\tau)\)</span> before and after <span class="math inline">\(n_{change}\)</span>. Log-transformation is used to ensure that the <span class="math inline">\(\tau\)</span> resulting from <span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_2\)</span> is positive.</p>
<p>We have to define priors for all parameters that are not specified by data.</p>
<p><span class="math inline">\(\alpha_1 \sim N(\mu = 0, \tau = 10^{-4})\)</span> That is a normal distribution with mean <span class="math inline">\(\mu = 0\)</span> and standard deviation <span class="math inline">\(\sigma = 100\)</span>,<br />
because <span class="math inline">\(\sigma = 1/\sqrt{\tau}\)</span><br />
<span class="math inline">\(\alpha_2 \sim N(0, 10^{-4})\)</span></p>
<p><span class="math inline">\(\beta_1 \sim N(0, 10^{-4})\)</span><br />
<span class="math inline">\(\beta_2 \sim N(0, 10^{-4})\)</span></p>
<p><span class="math inline">\(\log(\tau_1) \sim N(0, 10^{-4})\)</span><br />
<span class="math inline">\(\log(\tau_2) \sim N(0, 10^{-4})\)</span></p>
<p>Discrete prior on the change point. <span class="math inline">\(K\)</span> indicates one of the possible change points,
based on the probability vector <span class="math inline">\(p\)</span>, which we need to specify beforehand.</p>
</div>
<pre class="r"><code>model_CPR &lt;- function(){
  
  ### Likelihood or data model part
  for(i in 1:n){
    
  y[i] ~ dnorm(mu[i], tau[i]) 

    
    
  mu[i] &lt;- alpha_1 + 
  alpha_2 * step(i - n_change) +
  (beta_1 + beta_2 * step(i - n_change))*x[i]
  
  
  
  tau[i] &lt;- exp(log_tau[i])
  
  log_tau[i] &lt;- log_tau_1 + log_tau_2 * 
  step(i - n_change)
  } 
  
  ### Priors
  
  
  alpha_1 ~ dnorm(0, 1.0E-4)
  
  
  alpha_2 ~ dnorm(0, 1.0E-4)
  
  beta_1 ~ dnorm(0, 1.0E-4)
  beta_2 ~ dnorm(0, 1.0E-4)
  
  log_tau_1 ~ dnorm(0, 1.0E-4)
  log_tau_2 ~ dnorm(0, 1.0E-4)
  
  K ~ dcat(p)
  n_change &lt;- possible_change_points[K]

}</code></pre>
<p>Note that we put priors on <span class="math inline">\(\log(\tau_1)\)</span> and <span class="math inline">\(\log(\tau_2)\)</span>, rather than on <span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_2\)</span> directly, to ensure that the precision <span class="math inline">\(\tau\)</span> in the second part of the regression always remains positive. <span class="math inline">\(e^{\log(\tau_1) + \log(\tau_2)}\)</span> is always <span class="math inline">\(&gt; 0\)</span>, even if the term <span class="math inline">\(\log(\tau_1)\)</span> + <span class="math inline">\(\log(\tau_2)\)</span> becomes negative.</p>
<p>Prepare the data which we pass to JAGS along with the model:</p>
<pre class="r"><code># minimum number of the data points before and after the change
  min_segment_length &lt;- 5 

# assign indices to the potential change points we allow
  possible_change_points &lt;- (1:n)[(min_segment_length+1):(n+1-min_segment_length)] 
 
# number of possible change points
  M &lt;- length(possible_change_points)  

# probabilities for the discrete uniform prior on the possible change points, 
# i.e. all possible change points have the same prior probability
  p &lt;- rep(1 / M, length = M) 
 
# save the data to a list for jags
  data_CPR &lt;- list(&quot;x&quot;, &quot;y&quot;, &quot;n&quot;, &quot;possible_change_points&quot;, &quot;p&quot;) </code></pre>
<p>Load the <em>R2jags</em> package to access <em>JAGS</em> in <em>R</em>:</p>
<pre class="r"><code>  library(R2jags) </code></pre>
<p>Now we execute the change point regression. We instruct JAGS to run three seperate chains so we can verify that the results are consistent. We allow 2000 iterations of the Markov chain Monte Carlo algorithm for each chain, the first 1000 of which will automatically be discarded as burn-in.</p>
<pre class="r"><code> CPR  &lt;- jags(data = data_CPR, 
                         parameters.to.save = c(&quot;alpha_1&quot;, &quot;alpha_2&quot;, 
                                                &quot;beta_1&quot;,&quot;beta_2&quot;,
                                                &quot;log_tau_1&quot;,&quot;log_tau_2&quot;,
                                                &quot;n_change&quot;), 
                         n.iter = 2000, 
                         n.chains = 3,
                         model.file = model_CPR)</code></pre>
</div>
<div id="the-results" class="section level2">
<h2>The results</h2>
<p>To visualise the results and inspect the posterior, we are using the <em>ggmcmc</em> package, which relies on the <em>ggplot2</em> package. For brevity, we just look at the <span class="math inline">\(n_{change}\)</span> parameter here.</p>
<pre class="r"><code>library(ggmcmc)
CPR.ggs &lt;- ggs(as.mcmc(CPR)) # convert to ggs object
ggs_traceplot(CPR.ggs, family = &quot;n_change&quot;) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="1000 %" /></p>
<p>Looks like the chains converge and mix nicely. We can already see that our model locates the change point somewhere between <span class="math inline">\(30\)</span> and <span class="math inline">\(40\)</span>, although the chains occasionally explore regions further away.</p>
<p>Let’s look at the posterior probabilities for the possible change points:</p>
<pre class="r"><code>ggplot(data = CPR.ggs %&gt;% filter(Parameter == &quot;n_change&quot;),
  aes(x=value, y = 3*(..count..)/sum(..count..), fill = as.factor(Chain))) + 
  geom_vline(xintercept = 35,lty = 2) + geom_bar(position = &quot;identity&quot;, alpha = 0.5) +
  ylab(&quot;posterior probability&quot;) + xlab(&quot;n_change&quot;) + labs(fill=&#39;Chain&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="700 %" /></p>
<p>The <span class="math inline">\(37^{th}\)</span> point has the highest probability of being the change point. That is not far off from where we introduced the change, at the <span class="math inline">\(35^{th}\)</span> point (dashed line). The random generation of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> has led to <span class="math inline">\(37\)</span> being favoured. We also note that there are only minor differences between the three chains, and those differences would likely further dwindle if we were to let the chains run for longer.</p>
<p>Using the posterior distribution, we can answer questions like: “In which interval does the change point fall with 90 % probability?”</p>
<pre class="r"><code>quantile(CPR$BUGSoutput$sims.list$n_change, probs = c(0.05, 0.95))</code></pre>
<pre><code>##  5% 95% 
##  33  39</code></pre>
<p>We can also inquire about the probability that the change point falls in the interval <span class="math inline">\(34\)</span> to <span class="math inline">\(38\)</span>:</p>
<pre class="r"><code>round(length(which(CPR$BUGSoutput$sims.list$n_change %in% 34:38))/
              (CPR$BUGSoutput$n.sims),2)</code></pre>
<pre><code>## [1] 0.87</code></pre>
<p>Finally, let’s have a look at the regression parameters and plot the resulting regressions before and after the most likely change point.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" width="475 %" />
The intercept, slope, and residual variance all increase after the change point.</p>
<p>This can be immediately seen when plotting the change point regression:
<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="375 %" />
The shaded areas denote <span class="math inline">\(95\)</span> % credible intervals around the regression lines.</p>
<p>You can find the full R code for this analysis at <a href="https://github.com/KEichenseer/Methods/blob/main/Change_point_regression.R" class="uri">https://github.com/KEichenseer/Methods/blob/main/Change_point_regression.R</a></p>
<p>Get in touch if you have any comments or questions!</p>
</div>
